{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4z2zKIGrsv5gHOyMRihe4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2025.Q3-PLN/blob/main/2025_Q3_PLN_AULA_04_Notebook_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2025-Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmK05FgcOzL2"
      },
      "source": [
        "### **Tokens e Vocabulário**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em Processamento de Linguagem Natural (PLN), um ***token*** refere-se a uma unidade básica de processamento de texto. Geralmente, um ***token*** é definido como uma sequência contígua de caracteres que forma uma unidade significativa em um texto. Essa unidade pode ser uma **palavra**, um **número**, um **símbolo de pontuação** ou até mesmo um **caractere individual**, dependendo do objetivo do processamento.\n",
        "\n"
      ],
      "metadata": {
        "id": "YKeqSx2qOxPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por exemplo, na seguinte frase:\n",
        "\n",
        " > `O gato preto saltou sobre o muro`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FL-iWoxPUi3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os ***tokens*** são:\n",
        "\n",
        "> `O`, `gato`, `preto`, `saltou`, `sobre`, `o`, `muro`"
      ],
      "metadata": {
        "id": "xRoekKpTmlA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O **vocabulário**, por sua vez, refere-se ao conjunto de todos os *tokens* únicos que ocorrem em um determinado córpus. Em outras palavras, o **vocabulário** é o conjunto de todas as palavras distintas em um determinado conjunto de textos."
      ],
      "metadata": {
        "id": "BG-0lrj1PjrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O **vocabulário** é frequentemente usado em PLN para construir modelos estatísticos de linguagem, como modelos de linguagem de *n-gramas*, que podem ser usados ​​para tarefas como predição de palavras e geração de texto."
      ],
      "metadata": {
        "id": "4fsuJpafU5aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por exemplo, se tivermos um conjunto de documentos que contenha as seguintes frases:\n",
        "\n",
        "    O gato preto saltou sobre o muro.\n",
        "    O cão correu no parque.\n",
        "    O gato e o cão são animais de estimação populares."
      ],
      "metadata": {
        "id": "Xy6XIhyDPwq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantidade de ***tokens*** e ***types*** (tipos):\n",
        "\n",
        "> ***Tokens***: conta todas as ocorrências\n",
        "\n",
        "> ***Types***: conta apenas uma ocorrência = **tamanho do vocabulário**"
      ],
      "metadata": {
        "id": "NDxG0UD3WmCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O **vocabulário** desses documentos seria composto pelas seguintes ***types*** (tipos):\n",
        "\n",
        "`O`, `gato`, `preto`, `saltou`, `sobre`, `o`, `muro`, `cão`, `correu`, `no`, `parque`, `e`, `são`, `animais`, `de`, `estimação`, `populares`."
      ],
      "metadata": {
        "id": "n0t4IVbmWgMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que alguns ***tokens*** aparecem em mais de uma frase, como **`O`** e **`gato`**, mas o **vocabulário** inclui apenas uma única ocorrência de cada ***token***, ou seja, apenas os ***types***."
      ],
      "metadata": {
        "id": "GXgvu2DTQEWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenizadores**\n",
        "---"
      ],
      "metadata": {
        "id": "HMQGBMfNvWVa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI3GMSOv26F2"
      },
      "source": [
        "**Tokenizador por Espaço**\n",
        "\n",
        "Desenvolvido com base no principal delimitador para uma grande parcela das línguas naturais humanas: `o espaço`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBiqYr4kGlLE"
      },
      "source": [
        "texto = \"No meio do caminho tinha uma pedra.\"\n",
        "\n",
        "texto.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufM9ES0_3FwF"
      },
      "source": [
        "**Tokenizador baseado numa expressão regular**\n",
        "\n",
        "Segmenta as palavras de um texto com base em delimitadores como espaço, pontuações e início/fim de uma sequência (`\\b`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIllpjMcCIOs"
      },
      "source": [
        "import re\n",
        "\n",
        "texto = \"No meio do caminho tinha uma pedra.\"\n",
        "\n",
        "re.sub(r\"(\\b)\", r\" \\1\", texto).split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwGtvVJt3Nni"
      },
      "source": [
        "**Tokenizador baseado em Regras**\n",
        "\n",
        "1. Buscar todas as ocorrências de valores numéricos e financeiros (R$1,00; $46; etc.)\n",
        "\n",
        "2. Buscar todas as ocorrências de sequências de 1 ou mais caracteres\n",
        "\n",
        "3. Buscar todas as ocorrências de sequências sem espaço\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95ecY9vrFdph"
      },
      "source": [
        "texto = \"Eu paguei R$456,00 pelo setup. O que acha?\"\n",
        "regex = r\"R?\\$?[\\d\\.\\,]+|\\w+|\\S+\"\n",
        "re.findall(regex, texto)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de código em `Python` que usa a biblioteca `NLTK` para tokenizar uma frase:"
      ],
      "metadata": {
        "id": "lNRetsXkQl4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# faz o download do tokenizador punkt da biblioteca nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "iQe36eIlQzWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código **`nltk.download('punkt')`** é usado para fazer o download do **tokenizador** **`punkt`** da biblioteca **`NLTK`** (*`Natural Language Toolkit`*).\n",
        "\n",
        "O **tokenizador** **`punkt`** é um algoritmo de tokenização pré-treinado para separar um texto em unidades menores, geralmente palavras ou sentenças, considerando regras específicas para diferentes idiomas."
      ],
      "metadata": {
        "id": "NPrTwgQIRGQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# definir a frase a ser tokenizada\n",
        "frase = \"Eu gosto de pizza de mussarela.\"\n",
        "\n",
        "# usar a funcao word_tokenize() para tokenizar a frase\n",
        "tokens = word_tokenize(frase)\n",
        "\n",
        "# imprimir os tokens\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "USSErUfTQLCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(tokens)"
      ],
      "metadata": {
        "id": "O7hTSKjk0mX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que a função **`word_tokenize`** separou a frase em uma lista de *tokens*, considerando pontuações e espaços em branco."
      ],
      "metadata": {
        "id": "u5RLBlhKRa3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de código em **`Python`** que usa o conceito de vocabulário para contar a frequência de cada token em um córpus:"
      ],
      "metadata": {
        "id": "4OfSfGFvRdIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# definir o corpus\n",
        "texto = \"O gato preto saltou sobre o muro. O cão correu no parque. O gato e o cão são animais de estimação populares.\"\n",
        "\n",
        "# tokenizar o corpus\n",
        "tokens = word_tokenize(texto)\n",
        "\n",
        "# criar vocabulario a partir dos tokens [set: evitar duplicatas]\n",
        "vocabulario = set(tokens)"
      ],
      "metadata": {
        "id": "kp4LqNEnRtFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# criar dicionario para contar frequencia de cada token\n",
        "word_frequency = {}\n",
        "for word in tokens:\n",
        "    if word not in word_frequency:\n",
        "        word_frequency[word] = 1\n",
        "    else:\n",
        "        word_frequency[word] += 1"
      ],
      "metadata": {
        "id": "nnO8e2EBZkNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir vocabulario\n",
        "print(vocabulario)"
      ],
      "metadata": {
        "id": "2IKsDtv_ZfAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocabulario))"
      ],
      "metadata": {
        "id": "yDdp_onA1EkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir frequencia\n",
        "print(word_frequency)"
      ],
      "metadata": {
        "id": "d1QBiNJMaGFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo que usa a biblioteca **`NLTK`** para tokenizar um texto em palavras e calcular a frequência de cada palavra no texto."
      ],
      "metadata": {
        "id": "XMn5y5gyS6sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# definir o texto a ser processado\n",
        "texto = \"O Brasil é o país do futebol. O futebol é um esporte popular em todo o mundo.\"\n",
        "\n",
        "# tokenizar o texto em palavras\n",
        "palavras = word_tokenize(texto)\n",
        "\n",
        "# calcular a frequencia de cada palavra no texto\n",
        "frequencia = Counter(palavras)\n",
        "\n",
        "# imprimir as 5 palavras mais frequentes no texto\n",
        "print(frequencia.most_common(5))"
      ],
      "metadata": {
        "id": "QjChZit4SlPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está outro exemplo usando a biblioteca **`NLTK`**, que é similar ao exemplo anterior, mas usa a função **`nltk.FreqDist()`** para calcular as frequências de cada palavra."
      ],
      "metadata": {
        "id": "fhtJ--GzUMjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# definir o texto a ser processado\n",
        "texto = \"O Brasil é o país do futebol. O futebol é um esporte popular em todo o mundo.\"\n",
        "\n",
        "# tokenizar o texto em palavras\n",
        "palavras = word_tokenize(texto, language='portuguese')\n",
        "\n",
        "# calcular a frequencia de cada palavra no texto\n",
        "frequencia = FreqDist(palavras)\n",
        "\n",
        "# imprimir as 5 palavras mais frequentes no texto\n",
        "print(frequencia.most_common(5))"
      ],
      "metadata": {
        "id": "ZQOgroVwUHzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **NLTK**"
      ],
      "metadata": {
        "id": "tHnlSBeDUzbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A biblioteca **`NLTK`** (*Natural Language Toolkit*)  é uma biblioteca de PLN em **`Python`**. Ela é amplamente usada em pesquisas acadêmicas e industriais, e também é uma das bibliotecas mais populares em PLN.\n",
        "\n",
        "A biblioteca **`NLTK`** contém um grande conjunto de ferramentas e recursos para processar e analisar textos em linguagem natural. Alguns dos recursos mais importantes da biblioteca são:\n",
        "\n",
        "   * **Tokenização**: a biblioteca fornece várias opções para tokenizar textos em palavras e sentenças. A tokenização é a primeira etapa do pré-processamento de dados em PLN.\n",
        "\n",
        "   * ***POS (Part-of-Speech)*** ***Tagging***: a biblioteca possui algoritmos para marcar as palavras em um texto com sua classe gramatical, como substantivos, verbos, adjetivos etc.\n",
        "\n",
        "   * **Análise sintática**: a biblioteca pode ser usada para analisar a estrutura sintática de uma frase, como a identificação do sujeito, objeto, verbo principal etc.\n",
        "\n",
        "   * **Dicionários e corpora**: a biblioteca fornece acesso a vários dicionários e corpora em linguagem natural, que podem ser usados para treinar modelos de PLN ou como referência em pesquisas."
      ],
      "metadata": {
        "id": "e7N9w27eWOca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma das primeiras coisas a se testar é se o **`NLTK`** está disponível no **Google Colab**. Para isso, execute a seguinte instrução:"
      ],
      "metadata": {
        "id": "x_Y_AxstU0Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "aEkOK0scVA2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`NLTK` possui uma série de pacotes adicionais ou [corpora](#myfootnote1)  que podem ser facilmente adicionados na instalação básica da biblioteca.\n",
        "\n",
        "Para ter acesso ao download destes pacotes adicionais, basta utilizar o comando:\n",
        "\n",
        "`nltk.download()`\n",
        "\n",
        "<a name=\"myfootnote1\">Corpora</a>: plural de **córpus**, que em latim significa conjunto de uma obra. Termo que serve para designar um conjunto de textos, ou registros orais de uma língua para fins de análise."
      ],
      "metadata": {
        "id": "jzNlBBKLVHOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "id": "MhQHxzd0VTUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um dos **corpora** disponibilizado pela biblioteca `NLTK` é a obra completa de **Machado de Assis**. O nome desse pacote é \"`machado`\"."
      ],
      "metadata": {
        "id": "8xcWLpvvVsbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import machado"
      ],
      "metadata": {
        "id": "NFMTH6B8Vz2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aparentemente está disponível, mas caso não estivesse, bastaria realizar o download."
      ],
      "metadata": {
        "id": "25cayLuOV4EJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"machado\")"
      ],
      "metadata": {
        "id": "KnKGePnOV5Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora é possível, por exemplo, obter algumas informações sobre o córpus:"
      ],
      "metadata": {
        "id": "ctnGkFroV_iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import machado\n",
        "from nltk.corpus.util import LazyCorpusLoader\n",
        "\n",
        "# carregando o corpus usando LazyCorpusLoader com codificação ISO-8859-1\n",
        "machado_corpus = LazyCorpusLoader(\n",
        "    'machado',\n",
        "    CategorizedPlaintextCorpusReader,\n",
        "    r'(?!\\.).*\\.txt',\n",
        "    cat_pattern=r'([a-z]+)/.*',\n",
        "    encoding='latin1'\n",
        ")\n",
        "\n",
        "print (\"Número de arquivos no corpora:\", len(machado_corpus.fileids()))\n",
        "print (\"Primeiros três textos do corpora\", machado_corpus.fileids()[0:3])\n",
        "print (\"Quantas palavras existem nesse corpora?\", len(machado_corpus.words()))"
      ],
      "metadata": {
        "id": "WLSNuMDPqTvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outras bibliotecas**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "i6a9JXvjX13V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spaCy**"
      ],
      "metadata": {
        "id": "2g8hwV-1dn2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de como realizar a tokenização e criação do vocabulário usando a biblioteca `spaCy`:"
      ],
      "metadata": {
        "id": "4b5f5hAJX4Yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# as vezes eh necessario reiniciar o ambiente\n",
        "!python -m spacy download pt_core_news_lg"
      ],
      "metadata": {
        "id": "Gff8lCvTZ7EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de como realizar a tokenização e criação do vocabulário usando a biblioteca `spaCy`:"
      ],
      "metadata": {
        "id": "9lEnWTZMaH1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# carregando modelo do spaCy para a lingua portuguesa\n",
        "nlp = spacy.load(\"pt_core_news_lg\")\n",
        "\n",
        "# definindo texto para ser processado\n",
        "texto = \"O Brasil é o país do futebol. O futebol é um esporte popular em todo o mundo.\"\n",
        "\n",
        "# criando um objeto Doc para o texto\n",
        "doc = nlp(texto)\n",
        "\n",
        "# imprimindo tokens e seus indices\n",
        "for token in doc:\n",
        "    print(token.text, token.idx)"
      ],
      "metadata": {
        "id": "9qwQ4yexYCJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# criando o vocabulario a partir do objeto Doc\n",
        "vocabulario = set(token.text for token in doc)\n",
        "\n",
        "# imprimindo o vocabulario\n",
        "print(vocabulario)"
      ],
      "metadata": {
        "id": "9qRpbySdeND5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TextBlob**"
      ],
      "metadata": {
        "id": "1SZnJ1DWea6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de tokenização e criação de vocabulário usando a biblioteca `TextBlob`:"
      ],
      "metadata": {
        "id": "acM8w0eaali7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# definindo o texto de exemplo\n",
        "texto = \"O cachorro correu no parque. O gato dormiu na cozinha.\"\n",
        "\n",
        "# criando um objeto TextBlob\n",
        "blob = TextBlob(texto)\n",
        "\n",
        "# tokenizacao\n",
        "tokens = blob.words\n",
        "\n",
        "# Imprimindo os tokens\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "cfOipFm0bfxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construindo o vocabulario\n",
        "vocabulario = set(tokens)\n",
        "\n",
        "# Imprimindo o vocabulario\n",
        "print(vocabulario)"
      ],
      "metadata": {
        "id": "R8c5aDLJekbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo simples de tokenização de sentenças utilizando a biblioteca `TextBlob`:"
      ],
      "metadata": {
        "id": "XuSi1FDdbzFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# definindo o texto de exemplo\n",
        "texto = \"O cachorro correu no parque. O gato dormiu na cozinha.\"\n",
        "\n",
        "# criando um objeto TextBlob\n",
        "blob = TextBlob(texto)\n",
        "\n",
        "# tokenizacao de sentencas\n",
        "sentencas = blob.sentences"
      ],
      "metadata": {
        "id": "QHki7Zrzb0RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimindo as sentencas\n",
        "print(sentencas)"
      ],
      "metadata": {
        "id": "WyZnJbUjfFGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimindo as sentencas\n",
        "for sentenca in sentencas:\n",
        "  print(sentenca)"
      ],
      "metadata": {
        "id": "dhmIH2RNfLUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformers**"
      ],
      "metadata": {
        "id": "elf-h3L2fY0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Transformers` é um pacote do `Python` que fornece uma API para trabalhar com modelos de linguagem baseados em redes neurais, especialmente modelos de linguagem pré-treinados. Foi desenvolvido pela **Hugging Face** e é construído em cima da biblioteca `PyTorch`."
      ],
      "metadata": {
        "id": "5wLO5j0rNc5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hugging Face** é uma empresa de tecnologia sediada em Nova York que se concentra em tecnologias de PLN. Eles são mais conhecidos por seus pacotes `Transformers` e `Datasets`, que se tornaram muito populares na comunidade de PLN.\n",
        "\n",
        "O **Hugging Face** se concentra em criar ferramentas e tecnologias para tornar o PLN mais acessível a todos. Eles oferecem uma ampla gama de modelos de linguagem pré-treinados em várias línguas e especializados em várias tarefas, como tradução, resumo de texto, classificação de texto e geração de texto. Eles também desenvolveram uma API fácil de usar e amigável para desenvolvedores para trabalhar com esses modelos, incluindo o pacote `Transformers`."
      ],
      "metadata": {
        "id": "_quW2ZcNOPyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de como usar o `Transformers` para tokenizar um córpus:"
      ],
      "metadata": {
        "id": "Nt_4GdF9H0Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "vfm8MXNtGXmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# criar um novo tokenizador\n",
        "tokenizador = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# treinar o tokenizador em um corpus\n",
        "corpus = [\"Esta é a primeira frase.\", \"Esta é a segunda frase.\"]\n",
        "tokenizador.add_tokens(corpus)"
      ],
      "metadata": {
        "id": "SdgYsF1HG_5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# codificar uma string de texto\n",
        "text = \"Esta é a terceira frase.\"\n",
        "encoded = tokenizador.encode(text, add_special_tokens=False)\n",
        "tokens = tokenizador.convert_ids_to_tokens(encoded)"
      ],
      "metadata": {
        "id": "z96w2iD6f95A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Texto: {text}\")\n",
        "print(f\"Tokens: {tokens}\")"
      ],
      "metadata": {
        "id": "Nyb_i8MvgEIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decodificar a string de texto codificada\n",
        "decoded = tokenizador.decode(encoded)\n",
        "print(decoded)"
      ],
      "metadata": {
        "id": "0oN4amvCgAgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste exemplo, o método `encode` é usado com o argumento `add_special_tokens=False` para não incluir os *tokens* especiais adicionados pelo tokenizador (como [`CLS`] e [`SEP`]) na lista de *tokens*.\n",
        "\n"
      ],
      "metadata": {
        "id": "nMfUF9MKMNLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Em seguida, o método `convert_ids_to_tokens` é usado para converter os IDs de *token* codificados de volta em sua forma textual."
      ],
      "metadata": {
        "id": "1bO_nXGhgn3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O caractere \"#\" que você está vendo nos *tokens* representa o início de uma nova subpalavra que foi dividida a partir de uma palavra original durante a tokenização. Isso ocorre porque muitos modelos de linguagem, incluindo BERT, usam uma técnica chamada \"tokenização por subpalavra\" (*subword tokenization*) para lidar com palavras desconhecidas ou raras que não aparecem no vocabulário do modelo.\n",
        "\n"
      ],
      "metadata": {
        "id": "1wyl8E-GMUzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **tokenização por subpalavra** divide palavras em subpalavras menores que são mais comuns no córpus de treinamento. Por exemplo, a palavra \"gatos\" pode ser dividida em duas subpalavras, \"gat\" e \"##os\", onde \"##\" indica que a subpalavra faz parte de uma palavra maior.\n",
        "\n"
      ],
      "metadata": {
        "id": "NOiykJnBgyor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essa técnica permite que o modelo de linguagem lide com palavras que não estão presentes em seu vocabulário e capture melhor a semântica das palavras. No entanto, o resultado é que as subpalavras podem aparecer com o caractere \"#\" na frente para indicar que são partes de uma palavra maior."
      ],
      "metadata": {
        "id": "X39fvrwMg6oI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# criar um tokenizador do modelo BERT pre-treinado\n",
        "tokenizador = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# texto de exemplo para tokenizar\n",
        "texto = \"O cachorro correu feliz no parque.\"\n",
        "\n",
        "# tokenizar o texto\n",
        "tokens = tokenizador.tokenize(texto)"
      ],
      "metadata": {
        "id": "my6spvoFJQ8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "id": "uly85LQ3hKZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# criar um vocabulario usando o tokenizador\n",
        "vocabulario = tokenizador.get_vocab()"
      ],
      "metadata": {
        "id": "t0UHw01qhPOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(vocabulario)"
      ],
      "metadata": {
        "id": "LMYzp9FkjEA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tamanho do vocabulario\n",
        "print(len(vocabulario))"
      ],
      "metadata": {
        "id": "-p4-HtexiWFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "\n",
        "print(list(vocabulario.items())[0: k])"
      ],
      "metadata": {
        "id": "CCD9dIiLjg_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O vocabulário contém alguns *tokens* especiais, como [`PAD`], [`UNK`], [`CLS`], [`SEP`] e [`MASK`]. Esses *tokens* são usados pelo modelo BERT para tarefas como, por exemplo, Classificação de Textos."
      ],
      "metadata": {
        "id": "Wm7_u98-LY2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo mais interessante de como usar o tokenizador do pacote `transformers` para codificar um conjunto de frases e construir um vocabulário:"
      ],
      "metadata": {
        "id": "STv6WeuxOv9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# carregar um modelo pre-treinado com seu tokenizador\n",
        "modelo = \"bert-base-uncased\"\n",
        "tokenizador = AutoTokenizer.from_pretrained(modelo)\n",
        "\n",
        "# definir um conjunto de frases de exemplo\n",
        "sentencas = [\n",
        "    \"Esta é uma frase de exemplo.\",\n",
        "    \"Isso é um teste.\",\n",
        "    \"Você já experimentou isso antes?\",\n",
        "    \"Estou ansioso para o final de semana.\",\n",
        "    \"Não consigo parar de pensar nisso.\",\n",
        "    \"Que tempo maravilhoso hoje!\",\n",
        "]"
      ],
      "metadata": {
        "id": "82EMIqoqNwZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# codificar todas as frases do conjunto\n",
        "encoded_sentencas = [tokenizador.encode(sentenca, add_special_tokens=True) for sentenca in sentencas]\n",
        "\n",
        "# construir o vocabulario a partir do conjunto de frases codificadas\n",
        "vocabulario = set(tokenizador.convert_ids_to_tokens([token for sentenca in encoded_sentencas for token in sentenca]))"
      ],
      "metadata": {
        "id": "zXv50q2okedp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir os tokens de todas as frases do conjunto\n",
        "print(\"Tokens das frases:\")\n",
        "for i, sentenca in enumerate(sentencas):\n",
        "    print(f\"Frase {i+1}: {tokenizador.tokenize(sentenca)}\")"
      ],
      "metadata": {
        "id": "gBRhFi0KsIvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir tamanho do vocabulario\n",
        "print(\"Tamanho do Vocabulario:\")\n",
        "print(len(vocabulario))"
      ],
      "metadata": {
        "id": "EfvQ-ntSsK7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir vocabulario\n",
        "print(\"Vocabulário:\")\n",
        "print(vocabulario)"
      ],
      "metadata": {
        "id": "RUb_WgUns1FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stopwords**\n",
        "---"
      ],
      "metadata": {
        "id": "bxllvVx4PVIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Stopwords*** (palavras de parada, em português) são palavras comuns em um idioma que geralmente são removidas do texto durante o processamento de linguagem natural, pois elas não fornecem muita informação sobre o significado do texto. Essas palavras geralmente incluem artigos, preposições, conjunções e outras palavras comuns, como `o`, `a`, `de`, `para`, `com`, `em`, entre outras."
      ],
      "metadata": {
        "id": "5Ah3yyY5PkP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de *stopwords* em português usando a biblioteca `NLTK`:"
      ],
      "metadata": {
        "id": "0QJBb-PJP4qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "cfZ-cgu3QIWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = stopwords.words('portuguese')"
      ],
      "metadata": {
        "id": "6-xY6FNgQRsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(stopwords))"
      ],
      "metadata": {
        "id": "BAMNe2cdy9O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords)"
      ],
      "metadata": {
        "id": "OUHFClYGzIYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de remoção de *stopwords* usando a biblioteca `NLTK`:"
      ],
      "metadata": {
        "id": "SfZlY0-yRjID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = set(stopwords.words('portuguese'))\n",
        "\n",
        "texto = \"Eu gosto muito de estudar E aprender novos conhecimentos\"\n",
        "\n",
        "# remover stopwords e transformar para minusculo\n",
        "tokens = [token.lower() for token in texto.split() if token.lower() not in stopwords]"
      ],
      "metadata": {
        "id": "3uBbyrSLSHYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "id": "VsTVKvVQzcX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que as *stopwords* `Eu`, `muito`, `de` e `e` foram removidas da frase."
      ],
      "metadata": {
        "id": "awgCFw4aSc4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está uma exemplo de como adicionar novas *stopwords*:"
      ],
      "metadata": {
        "id": "b1CZ003G9yBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# criar uma lista com as stopwords em portugues\n",
        "stopwords = stopwords.words('portuguese')\n",
        "\n",
        "def remover_stopwords(texto, stopwords):\n",
        "    # dividir o texto em palavras\n",
        "    words = texto.split()\n",
        "\n",
        "    # remover as stopwords\n",
        "    words_clean = [word for word in words if word.lower() not in stopwords]\n",
        "\n",
        "    # junta as palavras novamente em uma string\n",
        "    text_clean = ' '.join(words_clean)\n",
        "\n",
        "    return text_clean"
      ],
      "metadata": {
        "id": "-AKzvABQ8Qgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exemplo de uso\n",
        "\n",
        "texto = \"O rato roeu a roupa do rei de Roma\"\n",
        "print(\"Texto original:\", texto)\n",
        "\n",
        "texto_limpo = remover_stopwords(texto, stopwords)\n",
        "print(\"Texto sem stopwords:\", texto_limpo)"
      ],
      "metadata": {
        "id": "KBnKl4OW0WjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adicionar novas stopwords\n",
        "stopwords_novas = [\"rei\", \"rato\"]\n",
        "stopwords.extend(stopwords_novas)\n",
        "\n",
        "texto_limpo = remover_stopwords(texto, stopwords)\n",
        "print(\"Texto sem stopwords e novas stopwords:\", texto_limpo)"
      ],
      "metadata": {
        "id": "dmoBydwu0oqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remover as novas stopwords\n",
        "stopwords = [word for word in stopwords if word not in stopwords_novas]\n",
        "\n",
        "# remover apenas as stopwords\n",
        "texto_limpo = remover_stopwords(texto, stopwords)\n",
        "print(\"Texto sem as novas stopwords:\", texto_limpo)"
      ],
      "metadata": {
        "id": "ncnO-OEa065E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spaCy**"
      ],
      "metadata": {
        "id": "o0HAzcas1n-Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Também é possível usar a biblioteca `spaCy` para remover *stopwords*. A seguir um exemplo que permite remover as *stopwords* em português e exibir as palavras restantes em uma nuvem de palavras."
      ],
      "metadata": {
        "id": "cNUGhHZcTXSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para isso, é necessário instalar o pacote `spaCy` e o modelo em português usando as seguintes instruções:"
      ],
      "metadata": {
        "id": "uJ0IzkrBTd_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "PgCvsQZaSf_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "A1DzNrH_2bZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depois disso, é possível usar o seguinte código `Python`:"
      ],
      "metadata": {
        "id": "DKh9lcfNTo-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.pt.stop_words import STOP_WORDS\n",
        "\n",
        "# exemplo de texto para remocao de stopwords\n",
        "texto = \"\"\"Em um lugar de destaque na Serra da Mantiqueira, cercado por montanhas e belas paisagens,\n",
        "o Hotel Parador Casa da Montanha oferece conforto, tranquilidade e lazer para toda a família.\n",
        "Com arquitetura inspirada nas montanhas, o hotel oferece suítes aconchegantes, restaurante com\n",
        "gastronomia de qualidade e diversas opções de lazer para crianças e adultos.\"\"\"\n",
        "\n",
        "# carregar o modelo em portugues\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "# remover as stopwords do texto\n",
        "tokens = nlp(texto)\n",
        "resultado = [token.text for token in tokens if not token.is_stop]"
      ],
      "metadata": {
        "id": "tNkstcp0THmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resultado)"
      ],
      "metadata": {
        "id": "J7bvxP-r29BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# criar a nuvem de palavras\n",
        "nuvem_palavras = WordCloud(width=800, height=500,\n",
        "                           background_color='white',\n",
        "                           random_state=42).generate(\" \".join(resultado))\n",
        "\n",
        "# exibir a nuvem de palavras\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(nuvem_palavras)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4hoxhZZ620dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A seguir é apresentado outro exemplo em que a biblioteca `NLTK` é usada para remover `stopwords` de um conjunto de tweets em português e criar um gráfico de barras com as palavras mais frequentes."
      ],
      "metadata": {
        "id": "PkofMAOaURUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "FhJWSmI4UGa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# exemplo de conjunto de tweets\n",
        "tweets = [\n",
        "    \"Acabei de assistir um filme incrível no cinema!\",\n",
        "    \"Hoje é um dia muito especial para mim. Estou muito feliz!\",\n",
        "    \"Preciso estudar para a prova de matemática que será amanhã.\",\n",
        "    \"Comprei um livro novo que estou ansioso para ler.\",\n",
        "    \"Estou cansado de ficar em casa o tempo todo. Preciso sair um pouco.\"\n",
        "]\n",
        "\n",
        "# carregar as stopwords em portugues\n",
        "stopwords = set(stopwords.words('portuguese'))\n",
        "\n",
        "# tokenizar e removee as stopwords dos tweets\n",
        "palavras = []\n",
        "for tweet in tweets:\n",
        "    tokens = nltk.word_tokenize(tweet.lower())\n",
        "    palavras.extend([token for token in tokens if not token in stopwords])\n",
        "\n",
        "# contar a frequencia das palavras\n",
        "frequencia = Counter(palavras)\n",
        "\n",
        "# selecionar as 10 palavras mais frequentes\n",
        "mais_frequentes = frequencia.most_common(10)"
      ],
      "metadata": {
        "id": "M1cCbxf9T_PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mais_frequentes)"
      ],
      "metadata": {
        "id": "sTe79PYX3xtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# criar um grafico de barras com as palavras mais frequentes\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar([palavra[0] for palavra in mais_frequentes], [palavra[1] for palavra in mais_frequentes],\n",
        "        width=0.4)\n",
        "plt.title(\"Palavras mais frequentes nos tweets\")\n",
        "plt.xlabel(\"Palavras\")\n",
        "plt.ylabel(\"Frequência\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bCXupnyH3qSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de como remover *stopwords* em inglês usando a biblioteca `NLTK` em `Python`."
      ],
      "metadata": {
        "id": "_8i72AuuWXrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "words = text.lower().split()\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words_without_stopwords = [word for word in words if word not in stop_words]\n",
        "\n",
        "print(words_without_stopwords)"
      ],
      "metadata": {
        "id": "VhTlq23VWF1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui está um exemplo de como remover as *stopwords* em português e em inglês usando o `NLTK`:"
      ],
      "metadata": {
        "id": "PmzVC-27-qGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# definir as stopwords em portugues\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "\n",
        "# definir as stopwords em ingles\n",
        "stopwords_en = set(stopwords.words('english'))\n",
        "\n",
        "# exemplo de texto em dois idiomas\n",
        "texto = \"\"\"The hotel room was great, but the breakfast could have been better.\n",
        "           O quarto do hotel era ótimo, mas o café da manhã poderia ter sido melhor.\"\"\"\n",
        "\n",
        "# remover as stopwords correspondentes aos dois idiomas\n",
        "palavras = [palavra for palavra in texto.split() if palavra.lower() not in stopwords_pt and palavra.lower() not in stopwords_en]"
      ],
      "metadata": {
        "id": "ST0mT4jQAP9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir as palavras sem as stopwords em portugues e ingles\n",
        "print(\"Texto sem stopwords:\", \" \".join(palavras))"
      ],
      "metadata": {
        "id": "Z7d1pg5K4Vum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detecção de Idiomas**"
      ],
      "metadata": {
        "id": "J8w3aA524j4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "É possível usar a biblioteca `langdetect` para identificar o idioma  e em seguida remover as *stopwords* correspondentes ao idioma. Aqui está um exemplo de como fazer isso:"
      ],
      "metadata": {
        "id": "uJnZQB0W_1t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "0R8KMRir--xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from langdetect import detect\n",
        "\n",
        "# definir as stopwords em portugues\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "\n",
        "# definir as stopwords em ingles\n",
        "stopwords_en = set(stopwords.words('english'))\n",
        "\n",
        "# exemplo de texto em dois idiomas\n",
        "avaliacoes = [\"The hotel room was great, but the breakfast could have been better.\",\n",
        "               \"O quarto do hotel era ótimo, mas o café da manhã poderia ter sido melhor.\"]"
      ],
      "metadata": {
        "id": "lDDrQt0Z-s7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identificar os idiomas\n",
        "for avaliacao in avaliacoes:\n",
        "  print(detect(avaliacao))"
      ],
      "metadata": {
        "id": "BvqmadiT6E-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palavras = []\n",
        "\n",
        "# remover as stopwords correspondentes ao idioma\n",
        "for avaliacao in avaliacoes:\n",
        "\n",
        "   idioma = detect(avaliacao)\n",
        "\n",
        "   if idioma == 'pt':\n",
        "        for palavra in avaliacao.split():\n",
        "           if palavra.lower() not in stopwords_pt:\n",
        "              palavras.append(palavra)\n",
        "   elif idioma == 'en':\n",
        "        for word in avaliacao.split():\n",
        "           if word.lower() not in stopwords_en:\n",
        "              palavras.append(word)\n",
        "   else:\n",
        "      print(\"Idioma não suportado.\")"
      ],
      "metadata": {
        "id": "i5wVn1DR5Qyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir as palavras sem as stopwords\n",
        "print(palavras)"
      ],
      "metadata": {
        "id": "EIrivCmE6jf-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}