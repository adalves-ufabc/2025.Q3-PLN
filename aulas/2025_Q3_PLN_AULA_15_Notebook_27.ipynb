{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2025.Q3-PLN/blob/main/2025_Q3_PLN_AULA_15_Notebook_27.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2025-Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmK05FgcOzL2"
      },
      "source": [
        "## **ExtraÃ§Ã£o de InformaÃ§Ã£o [LangChain]**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYKEbnlNTVlR"
      },
      "outputs": [],
      "source": [
        "#@title Instalando o pacote LangChain\n",
        "!pip install langchain -q U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJvs5RTcbE64",
        "outputId": "ca353c6f-cf60-41f3-d46d-fce83dae3d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3.27\n"
          ]
        }
      ],
      "source": [
        "#@title VersÃ£o do LangChain\n",
        "\n",
        "import langchain\n",
        "\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title IntegraÃ§Ã£o com o pacote da OpenAI\n",
        "\n",
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "8klfbjqKbUpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2c1135-1b4f-46b1-a1a5-a2ce4495727c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.9/469.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM5RnKClbPtQ",
        "outputId": "0902a452-81c8-4a11-cc17-0b41b9caaca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "#@title Definindo a chave da API da OpenAI\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Definindo a chave da API da OpenAI\n",
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq1uDRMB12YU",
        "outputId": "3968c018-ff0a-482f-9a9d-a3c59226ec76"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ExtraÃ§Ã£o de InformaÃ§Ã£o**"
      ],
      "metadata": {
        "id": "48inXIgDTxlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ExtraÃ§Ã£o de InformaÃ§Ã£o** (EI) Ã© o processo de identificar e extrair dados estruturados a partir de texto nÃ£o estruturado. O objetivo Ã© transformar o texto bruto em informaÃ§Ãµes organizadas que podem ser facilmente analisadas e usadas para outras tarefas, como a construÃ§Ã£o de bases de dados ou a realizaÃ§Ã£o de anÃ¡lises."
      ],
      "metadata": {
        "id": "_OfEFH6c4hjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Construir uma cadeia de extraÃ§Ã£o**"
      ],
      "metadata": {
        "id": "-lpmLOvr7s7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Schema***\n",
        "\n",
        "Primeiro, precisamos descrever quais informaÃ§Ãµes queremos extrair do texto.\n",
        "\n",
        "Usaremos o `Pydantic` para definir um esquema de exemplo para extrair informaÃ§Ãµes pessoais."
      ],
      "metadata": {
        "id": "ln2MlIfq77_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Pessoa(BaseModel):\n",
        "    \"\"\"InformaÃ§Ãµes sobre uma pessoa.\"\"\"\n",
        "\n",
        "    # ^ Doc-string para a entidade Pessoa.\n",
        "    # Este doc-string Ã© enviado ao LLM como a descriÃ§Ã£o do esquema Pessoa,\n",
        "    # e pode ajudar a melhorar os resultados da extraÃ§Ã£o.\n",
        "\n",
        "    # Observe que:\n",
        "    # 1. Cada campo Ã© `opcional` -- isso permite que o modelo opte por nÃ£o extraÃ­-lo!\n",
        "    # 2. Cada campo tem uma `descriÃ§Ã£o` -- essa descriÃ§Ã£o Ã© usada pelo LLM.\n",
        "    # Ter uma boa descriÃ§Ã£o pode ajudar a melhorar os resultados da extraÃ§Ã£o.\n",
        "    nome: Optional[str] = Field(default=None, description=\"O nome da pessoa\")\n",
        "    cor_cabelo: Optional[str] = Field(\n",
        "        default=None, description=\"A cor do cabelo da pessoa, se conhecida\"\n",
        "    )\n",
        "    altura_metros: Optional[str] = Field(\n",
        "        default=None, description=\"Altura medida em metros\"\n",
        "    )"
      ],
      "metadata": {
        "id": "eX2GKP2c8POV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui estÃ£o duas boas prÃ¡ticas ao definir um esquema:\n",
        "\n",
        "- Documente os atributos e o prÃ³prio esquema: essas informaÃ§Ãµes sÃ£o enviadas ao LLM e usadas para melhorar a qualidade da extraÃ§Ã£o de informaÃ§Ãµes.\n",
        "- NÃ£o force o LLM a inventar informaÃ§Ãµes! Acima, usamos `Optional` para os atributos, permitindo que o LLM retorne `None` se nÃ£o souber a resposta."
      ],
      "metadata": {
        "id": "ss2ElouD89N0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extrator**"
      ],
      "metadata": {
        "id": "OUdUpssO9Z_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos criar um extrator de informaÃ§Ãµes usando o esquema definido acima."
      ],
      "metadata": {
        "id": "4OcmsitU9bwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Defina um prompt personalizado para fornecer instruÃ§Ãµes e qualquer contexto adicional.\n",
        "# 1) VocÃª pode adicionar exemplos ao modelo de prompt para melhorar a qualidade da extraÃ§Ã£o.\n",
        "# 2) Introduza parÃ¢metros adicionais para levar o contexto em consideraÃ§Ã£o (por exemplo, incluir metadados\n",
        "#    sobre o documento do qual o texto foi extraÃ­do.)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"VocÃª Ã© um algoritmo de extraÃ§Ã£o especializado. \"\n",
        "            \"Extraia apenas as informaÃ§Ãµes relevantes do texto. \"\n",
        "            \"Se vocÃª nÃ£o souber o valor de um atributo solicitado para extraÃ§Ã£o, \"\n",
        "            \"retorne nulo para o valor do atributo.\",\n",
        "        ),\n",
        "        (\"human\", \"{texto}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "unMCaSKD9m_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisamos usar um modelo:"
      ],
      "metadata": {
        "id": "_fM9ynH8-BMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key = OPENAI_API_KEY)\n",
        "\n",
        "chain = prompt | modelo.with_structured_output( schema = Pessoa )"
      ],
      "metadata": {
        "id": "Jun5BBQK-D02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos testar isso:"
      ],
      "metadata": {
        "id": "ENDtnrt6-k_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"Alan Smith tem 183 centÃ­metros de altura e cabelo loiro.\"\n",
        "\n",
        "chain.invoke({\"texto\": texto})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2JTybn6-ift",
        "outputId": "96c0b4d7-52c6-4331-ffc4-d69a229eed2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pessoa(nome='Alan Smith', cor_cabelo='loiro', altura_metros='1.83')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interessante**\n",
        "\n",
        ">\n",
        "\n",
        "ExtraÃ§Ã£o Ã© Generativa ğŸ¤¯\n",
        ">\n",
        "Os LLMs sÃ£o modelos generativos, entÃ£o eles podem fazer coisas bastante interessantes, como extrair corretamente a altura da pessoa em metros, mesmo que tenha sido fornecida em outra unidade!"
      ],
      "metadata": {
        "id": "zSP3k9db-_q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MÃºltiplas Entidades**"
      ],
      "metadata": {
        "id": "76kyr3u6_ui-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na maioria dos casos, vocÃª deve estar extraindo uma lista de entidades em vez de uma Ãºnica entidade. Isso pode ser facilmente alcanÃ§ado usando o `Pydantic` ao aninhar modelos dentro de outros modelos."
      ],
      "metadata": {
        "id": "JUtLHhHJ_wsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Pessoa(BaseModel):\n",
        "    \"\"\"InformaÃ§Ãµes sobre uma pessoa.\"\"\"\n",
        "\n",
        "    # ^ Doc-string para a entidade Pessoa.\n",
        "    # Este doc-string Ã© enviado ao LLM como a descriÃ§Ã£o do esquema Pessoa,\n",
        "    # e pode ajudar a melhorar os resultados da extraÃ§Ã£o.\n",
        "\n",
        "    # Observe que:\n",
        "    # 1. Cada campo Ã© `opcional` -- isso permite que o modelo opte por nÃ£o extraÃ­-lo!\n",
        "    # 2. Cada campo tem uma `descriÃ§Ã£o` -- essa descriÃ§Ã£o Ã© usada pelo LLM.\n",
        "    # Ter uma boa descriÃ§Ã£o pode ajudar a melhorar os resultados da extraÃ§Ã£o.\n",
        "    nome: Optional[str] = Field(default=None, description=\"O nome da pessoa\")\n",
        "    cor_cabelo: Optional[str] = Field(\n",
        "        default=None, description=\"A cor do cabelo da pessoa, se conhecida\"\n",
        "    )\n",
        "    altura_metros: Optional[str] = Field(\n",
        "        default=None, description=\"Altura medida em metros\"\n",
        "    )\n",
        "\n",
        "class Dados(BaseModel):\n",
        "    \"\"\"Dados extraÃ­dos sobre pessoas.\"\"\"\n",
        "\n",
        "    # Cria um modelo para que possamos extrair vÃ¡rias entidades.\n",
        "    pessoas: List[Pessoa]"
      ],
      "metadata": {
        "id": "cnjE4iz4_93U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | modelo.with_structured_output( schema = Dados)\n",
        "\n",
        "texto = \"Meu nome Ã© JoÃ£o, meu cabelo Ã© preto e eu tenho 1,83 metros de altura. Ana tem a mesma cor de cabelo que eu.\"\n",
        "\n",
        "chain.invoke({\"texto\": texto})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQMlwIemA2aI",
        "outputId": "3cc5da23-e895-48d5-e824-620a73a91990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dados(pessoas=[Pessoa(nome='JoÃ£o', cor_cabelo='preto', altura_metros='1.83'), Pessoa(nome='Ana', cor_cabelo='preto', altura_metros=None)])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Como lidar com textos longos ao realizar a extraÃ§Ã£o**"
      ],
      "metadata": {
        "id": "i9aYk7O4R5-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando trabalhar com arquivos, como PDFs, Ã© provÃ¡vel que vocÃª encontre texto que excede a janela de contexto do seu modelo de linguagem. Para processar esse texto, considere estas estratÃ©gias:\n",
        "\n",
        "1. **Mudar o LLM**: escolha um LLM diferente que suporte uma janela de contexto maior.\n",
        "2. **ForÃ§a Bruta**: divida o documento em partes e extraia o conteÃºdo de cada parte.\n",
        "3. **RAG (*Retrieval-Augmented Generation*)**: divida o documento em partes, indexe as partes e extraia conteÃºdo apenas de um subconjunto de partes que pareÃ§am \"relevantes\".\n",
        "\n",
        "Lembre-se de que essas estratÃ©gias tÃªm diferentes compromissos, e a melhor estratÃ©gia provavelmente depende da aplicaÃ§Ã£o que vocÃª estÃ¡ projetando!\n",
        "\n",
        "Aqui iremos demonstrar como implementar as estratÃ©gias 2 e 3."
      ],
      "metadata": {
        "id": "UcbCbEhnbD9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_community"
      ],
      "metadata": {
        "id": "MtlqBDrZw0Iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ffbfac-c33c-4c92-d6f2-9b12ad2d9988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.3 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisamos de alguns dados de exemplo! Vamos baixar um artigo sobre carros da **Wikipedia** e carregÃ¡-lo como um `Document` do LangChain."
      ],
      "metadata": {
        "id": "mBT8Br5sbqI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import requests\n",
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "\n",
        "# define um cabeÃ§alho com um user-agent\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
        "                  'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
        "                  'Chrome/115.0.0.0 Safari/537.36'\n",
        "}\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/History_of_the_automobile'\n",
        "\n",
        "response = requests.get(url, headers = HEADERS)\n",
        "\n",
        "with open(\"carros.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "loader = BSHTMLLoader(\"carros.html\")\n",
        "documento = loader.load()[0]\n",
        "documento.page_content = re.sub(\"\\n\\n+\", \"\\n\", documento.page_content)"
      ],
      "metadata": {
        "id": "e6wsKEamwwpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documento.page_content[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "icIlFoyH5CHG",
        "outputId": "4aad6c19-bd61-4f9b-fc00-c31dce295e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nHistory of the automobile - Wikipedia\\nJump to content\\nMain menu\\nMain menu\\nmove to sidebar\\nhide\\n\\t\\tNavigation\\n\\t\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact us\\n\\t\\tContribute\\n\\t\\nHelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\\nSearch\\nSearch\\nAppearance\\nDonate\\nCreate account\\nLog in\\nPersonal tools\\nDonate Create account Log in\\nContents\\nmove to sidebar\\nhide\\n(Top)\\n1\\nPower sources\\nToggle Power sources subsection\\n1.1\\nSteam-powered wheeled vehicles\\n1.1.1\\n17th and 18th centuries\\n1.1.2\\n19th century\\n1.1.3\\n20th century\\n1.2\\nElectric automobiles\\n1.2.1\\n19th century\\n1.2.2\\n20th century\\n1.2.3\\n21st century\\n1.3\\nInternal combustion engines\\n1.3.1\\nGas mixtures\\n1.3.2\\nGasoline\\n2\\nEras of invention\\nToggle Eras of invention subsection\\n2.1\\nHorseless carriage or veteran era\\n2.2\\nBrass/Edwardian era\\n2.3\\nVintage era\\n2.4\\nPre-war era\\n2.5\\nPostwar era\\n2.6\\nModern era\\n3\\nSee also\\n4\\nReferences\\n5\\nFurther reading\\nToggle Further reading subsection\\n5.1\\nEarly sources\\n6\\nExternal links\\nToggle'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documento.page_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM0Q7J6Yw-96",
        "outputId": "eac82346-d950-49cf-8e4d-c331e413f4a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definir o esquema**"
      ],
      "metadata": {
        "id": "VQAqWNnlcYrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos o `Pydantic` para definir o esquema das informaÃ§Ãµes que desejamos extrair. Neste caso, extrair uma lista de principais desenvolvimentos (por exemplo, eventos histÃ³ricos importantes) que incluam um ano e uma descriÃ§Ã£o.\n",
        "\n",
        ">\n",
        "Observe que tambÃ©m incluÃ­mos uma chave de evidÃªncia e instruÃ­mos o modelo a fornecer, literalmente, as sentenÃ§as relevantes do texto do artigo. Isso nos permite comparar os resultados da extraÃ§Ã£o com (a reconstruÃ§Ã£o do modelo de) texto do documento original."
      ],
      "metadata": {
        "id": "YFGYT7dIcfA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Desenvolvimento(BaseModel):\n",
        "    \"\"\"InformaÃ§Ãµes sobre um desenvolvimento na histÃ³ria dos carros.\"\"\"\n",
        "\n",
        "    ano: int = Field(\n",
        "        ..., description=\"O ano em que houve um desenvolvimento histÃ³rico importante.\"\n",
        "    )\n",
        "    descricao: str = Field(\n",
        "        ..., description=\"O que aconteceu neste ano? Qual foi o desenvolvimento?\"\n",
        "    )\n",
        "    evidencia: str = Field(\n",
        "        ...,\n",
        "        description=\"Repita literalmente a(s) frase(s) da qual as informaÃ§Ãµes de ano e descriÃ§Ã£o foram extraÃ­das.\",\n",
        "    )\n",
        "\n",
        "\n",
        "class ExtracaoDados(BaseModel):\n",
        "    \"\"\"InformaÃ§Ãµes extraÃ­das sobre desenvolvimentos chave na histÃ³ria dos carros.\"\"\"\n",
        "\n",
        "    desenvolvimentos: List[Desenvolvimento]\n",
        "\n",
        "\n",
        "# Defina um prompt personalizado para fornecer instruÃ§Ãµes e qualquer contexto adicional.\n",
        "# 1) VocÃª pode adicionar exemplos no template do prompt para melhorar a qualidade da extraÃ§Ã£o.\n",
        "# 2) Introduza parÃ¢metros adicionais para considerar o contexto (por exemplo, incluir metadados\n",
        "#    sobre o documento do qual o texto foi extraÃ­do.)\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"VocÃª Ã© um especialista em identificar desenvolvimentos histÃ³ricos chave em textos. \"\n",
        "            \"Extraia apenas desenvolvimentos histÃ³ricos importantes. NÃ£o extraia nada se nÃ£o houver informaÃ§Ãµes importantes no texto.\",\n",
        "        ),\n",
        "        (\"human\", \"{texto}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "YW0OV_V_c7r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Criar um extrator**"
      ],
      "metadata": {
        "id": "umnkysVseJfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos selecionar um LLM. Como estamos usando chamadas de `ferramentas`, precisaremos de um modelo que suporte esse recurso."
      ],
      "metadata": {
        "id": "ZDsLPdaJeMLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "9ODWnJSteXrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extrator = prompt | modelo.with_structured_output(\n",
        "    schema = ExtracaoDados,\n",
        "    include_raw = False,\n",
        ")"
      ],
      "metadata": {
        "id": "5KABwirYeXto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abordagem de forÃ§a bruta**\n",
        "\n"
      ],
      "metadata": {
        "id": "nuxya9o3fG1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divida os documentos em partes (*chunks*) de modo que cada parte se encaixe na janela de contexto dos LLMs."
      ],
      "metadata": {
        "id": "pHZ5FhDTfKHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import TokenTextSplitter\n",
        "\n",
        "text_splitter = TokenTextSplitter(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=20,\n",
        ")\n",
        "\n",
        "textos = text_splitter.split_text(documento.page_content)"
      ],
      "metadata": {
        "id": "Y32AiFb_esRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `chunk_size=2000`: define o tamanho mÃ¡ximo de cada parte (*chunk*) em tokens. Neste caso, cada parte terÃ¡ no mÃ¡ximo 2000 tokens.\n",
        "\n",
        "* `chunk_overlap=20`: define o nÃºmero de tokens que se sobrepÃµem entre partes adjacentes. Uma sobreposiÃ§Ã£o de 20 tokens significa que os 20 tokens finais de uma parte serÃ£o incluÃ­dos no inÃ­cio da prÃ³xima parte. Isso ajuda a garantir que o contexto nÃ£o seja perdido entre partes consecutivas."
      ],
      "metadata": {
        "id": "U5c9TxefgAup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a funcionalidade de processamento em lote (`batch`) para executar a extraÃ§Ã£o em paralelo em cada parte!"
      ],
      "metadata": {
        "id": "ddv3gEkigiqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limite apenas Ã s primeiras 3 partes\n",
        "# para que o cÃ³digo possa ser executado rapidamente\n",
        "partes = textos[:3]\n",
        "\n",
        "resposta = extrator.batch(\n",
        "    [{\"texto\": texto} for texto in partes],\n",
        "    {\"max_concurrency\": 5},  # nÃºmero mÃ¡ximo de chamadas simultÃ¢neas\n",
        ")"
      ],
      "metadata": {
        "id": "LsJfOmuPgnZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mesclar resultados**\n",
        "\n",
        "ApÃ³s extrair dados das partes, serÃ¡ necessÃ¡rio mesclar as extraÃ§Ãµes."
      ],
      "metadata": {
        "id": "xYdCWtAyhNMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "desenvolvimentos = []\n",
        "\n",
        "for extracao in resposta:\n",
        "    desenvolvimentos.extend(extracao.desenvolvimentos)\n",
        "\n",
        "desenvolvimentos[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4dAz4DthLye",
        "outputId": "a145dbff-2552-41dc-c5db-1d004499251c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Desenvolvimento(ano=1649, descricao='Hans Hautsch de Nuremberg construiu uma carruagem movida a relÃ³gio.', evidencia='Em 1649, Hans Hautsch de Nuremberg construiu uma carruagem movida a relÃ³gio.'),\n",
              " Desenvolvimento(ano=1672, descricao='Ferdinand Verbiest criou um pequeno veÃ­culo movido a vapor.', evidencia='Em 1672, um pequeno veÃ­culo movido a vapor foi criado por Ferdinand Verbiest.'),\n",
              " Desenvolvimento(ano=1769, descricao='Nicolas-Joseph Cugnot construiu o primeiro automÃ³vel movido a vapor capaz de transportar humanos.', evidencia='O primeiro automÃ³vel movido a vapor capaz de transporte humano foi construÃ­do por Nicolas-Joseph Cugnot em 1769.'),\n",
              " Desenvolvimento(ano=1886, descricao='Carl Benz desenvolveu um automÃ³vel movido a gasolina e fez vÃ¡rias cÃ³pias idÃªnticas, marcando o inÃ­cio da produÃ§Ã£o em sÃ©rie.', evidencia='O primeiro carro modernoâ€”um automÃ³vel prÃ¡tico e comercializÃ¡vel para uso diÃ¡rioâ€”e o primeiro carro em produÃ§Ã£o em sÃ©rie apareceram em 1886, quando Carl Benz desenvolveu um automÃ³vel movido a gasolina e fez vÃ¡rias cÃ³pias idÃªnticas.'),\n",
              " Desenvolvimento(ano=1890, descricao='Gottlieb Daimler e Wilhelm Maybach fundaram a Daimler Motoren Gesellschaft.', evidencia='Em 1890, Gottlieb Daimler, inventor do motor de alta velocidade movido a petrÃ³leo lÃ­quido, e Wilhelm Maybach formaram a Daimler Motoren Gesellschaft.'),\n",
              " Desenvolvimento(ano=1926, descricao='A Daimler Motoren Gesellschaft se fundiu com a Benz & Cie para formar a Daimler-Benz.', evidencia='Em 1926, a empresa se fundiu com Benz & Cie. (fundada por Carl Benz em 1883) para formar a Daimler-Benz, conhecida pela sua marca de automÃ³veis Mercedes-Benz.'),\n",
              " Desenvolvimento(ano=1901, descricao='Ransom E. Olds fundou a Oldsmobile e introduziu o Curved Dash Oldsmobile.', evidencia='Ransom E. Olds fundou a Oldsmobile em 1897, e introduziu o Curved Dash Oldsmobile em 1901.'),\n",
              " Desenvolvimento(ano=1903, descricao='Oldsmobile comeÃ§ou a produzir milhares de veÃ­culos usando a linha de montagem.', evidencia='Olds pioneirou a linha de montagem usando peÃ§as idÃªnticas e intercambiÃ¡veis, produzindo milhares de Oldsmobiles atÃ© 1903.'),\n",
              " Desenvolvimento(ano=1908, descricao='A Ford Motor Company revolucionou a produÃ§Ã£o de automÃ³veis com o Ford Model T.', evidencia='Em 1908, a Ford Motor Company further revolutionized automobile production by developing and selling its Ford Model T at a relatively modest price.'),\n",
              " Desenvolvimento(ano=1913, descricao='A Ford introduziu uma linha de montagem avanÃ§ada, reduzindo o preÃ§o do Model T em quase 50%.', evidencia='A partir de 1913, introduzindo uma linha de montagem avanÃ§ada permitiu Ã  Ford reduzir o preÃ§o do Model T em quase 50%, tornando-o o primeiro automÃ³vel acessÃ­vel em massa.')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for desenvolvimento in sorted(desenvolvimentos, key=lambda x: x.ano):\n",
        "    print(f\"{desenvolvimento.ano}: {desenvolvimento.descricao}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj8kR8U0hqzA",
        "outputId": "ee543575-a625-4939-c181-8e8749f2f51d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1649: Hans Hautsch de Nuremberg construiu uma carruagem movida a relÃ³gio.\n",
            "1672: Ferdinand Verbiest criou um pequeno veÃ­culo movido a vapor.\n",
            "1769: Nicolas-Joseph Cugnot construiu o primeiro automÃ³vel movido a vapor capaz de transportar humanos.\n",
            "1879: A legislatura premiou metade do prÃªmio da corrida de veÃ­culos entre Green Bay e Oshkosh.\n",
            "1885: Benz construiu o primeiro automÃ³vel moderno, o Benz Patent Motorcar, considerado o primeiro carro prÃ¡tico e comercializÃ¡vel.\n",
            "1886: Carl Benz desenvolveu um automÃ³vel movido a gasolina e fez vÃ¡rias cÃ³pias idÃªnticas, marcando o inÃ­cio da produÃ§Ã£o em sÃ©rie.\n",
            "1886: Benz recebeu uma patente para seu automÃ³vel em 29 de janeiro.\n",
            "1888: Bertha Benz realizou a primeira viagem de longa distÃ¢ncia com um automÃ³vel, provando sua viabilidade.\n",
            "1888: Os primeiros automÃ³veis foram produzidos por Carl Benz na Alemanha e, sob licenÃ§a de Benz, na FranÃ§a por Emile Roger.\n",
            "1888: Bertha Benz realizou uma viagem de longa distÃ¢ncia, demonstrando a utilidade do automÃ³vel.\n",
            "1890: Gottlieb Daimler e Wilhelm Maybach fundaram a Daimler Motoren Gesellschaft.\n",
            "1893: Os irmÃ£os Charles e Frank Duryea fundaram a Duryea Motor Wagon Company, tornando-se a primeira empresa americana de fabricaÃ§Ã£o de automÃ³veis.\n",
            "1894: A corrida Paris-Rouen ocorreu, sendo descrita como a primeira corrida competitiva de automÃ³veis do mundo.\n",
            "1895: George B. Selden recebeu uma patente para um motor de automÃ³vel, que foi licenciado para a maioria dos fabricantes de automÃ³veis americanos.\n",
            "1901: Ransom E. Olds fundou a Oldsmobile e introduziu o Curved Dash Oldsmobile.\n",
            "1901: A Olds Motor Vehicle Company (mais tarde conhecida como Oldsmobile) iniciou a produÃ§Ã£o em linha do Oldsmobile Curved Dash.\n",
            "1903: Oldsmobile comeÃ§ou a produzir milhares de veÃ­culos usando a linha de montagem.\n",
            "1903: O Rambler padronizou o volante e moveu a posiÃ§Ã£o do motorista para o lado esquerdo do veÃ­culo.\n",
            "1903: Horatio Nelson Jackson completou uma viagem transcontinental bem-sucedida pelos EUA em um carro Winton.\n",
            "1905: O perÃ­odo Brass ou Edwardian comeÃ§ou, caracterizado pelo uso generalizado de latÃ£o nos veÃ­culos.\n",
            "1906: Os carros a vapor se tornaram alguns dos veÃ­culos mais rÃ¡pidos da Ã©poca.\n",
            "1908: A Ford Motor Company revolucionou a produÃ§Ã£o de automÃ³veis com o Ford Model T.\n",
            "1913: A Ford introduziu uma linha de montagem avanÃ§ada, reduzindo o preÃ§o do Model T em quase 50%.\n",
            "1914: O inÃ­cio da Primeira Guerra Mundial marcou o fim do perÃ­odo Brass ou Edwardian.\n",
            "1926: A Daimler Motoren Gesellschaft se fundiu com a Benz & Cie para formar a Daimler-Benz.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abordagem baseada em *RAG***"
      ],
      "metadata": {
        "id": "3lpUooYIicQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***RAG*** (*Retrieval-Augmented Generation* - GeraÃ§Ã£o aumentada de recuperaÃ§Ã£o)"
      ],
      "metadata": {
        "id": "VryuJbWgqgFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outra ideia simples Ã© dividir o texto em partes, mas em vez de extrair informaÃ§Ãµes de cada parte, concentre-se apenas nas partes mais relevantes."
      ],
      "metadata": {
        "id": "lsrI6FvxmIFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**AtenÃ§Ã£o**\n",
        "\n",
        "Pode ser difÃ­cil identificar quais partes sÃ£o relevantes.\n",
        "\n",
        "Por exemplo, no artigo sobre carros que estamos usando aqui, a maior parte do artigo contÃ©m informaÃ§Ãµes sobre desenvolvimentos chave. EntÃ£o, ao usar ***RAG***, provavelmente estaremos descartando muitas informaÃ§Ãµes relevantes.\n",
        "\n",
        "Sugerimos experimentar com seu caso de uso e determinar se essa abordagem funciona ou nÃ£o.\n",
        "\n",
        "Para implementar a abordagem baseada em ***RAG***:\n",
        "\n",
        "1. Divida seu(s) documento(s) em partes e indexe-os (por exemplo, em um `vectorstore`);\n",
        "2. Adicione um passo de recuperaÃ§Ã£o antes do extrator, utilizando o `vectorstore`."
      ],
      "metadata": {
        "id": "W52plXzTiCH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***RAG*** (*Retrieval-Augmented Generation*) Ã© uma abordagem que combina recuperaÃ§Ã£o de informaÃ§Ãµes com geraÃ§Ã£o de texto. A ideia principal Ã© melhorar a capacidade de um modelo de linguagem ao integrar informaÃ§Ãµes recuperadas de uma base de dados ou Ã­ndice com a geraÃ§Ã£o de texto."
      ],
      "metadata": {
        "id": "A6gHJpDfllgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como funciona o ***RAG***:\n",
        "\n",
        "   * **RecuperaÃ§Ã£o**: quando um modelo precisa gerar uma resposta ou extrair informaÃ§Ãµes, primeiro ele realiza uma busca em uma base de dados ou Ã­ndice para recuperar partes relevantes do texto ou documentos que podem ajudar a responder a pergunta ou completar a tarefa. Esta base de dados Ã© frequentemente um `vectorstore`, que armazena *embeddings* de texto para rÃ¡pida recuperaÃ§Ã£o.\n",
        "\n",
        "   * **GeraÃ§Ã£o**: apÃ³s recuperar as informaÃ§Ãµes relevantes, o modelo de linguagem utiliza essas informaÃ§Ãµes como contexto adicional para gerar uma resposta mais informada e precisa. Isso combina o conhecimento do modelo com dados especÃ­ficos recuperados, aumentando a relevÃ¢ncia e precisÃ£o da saÃ­da gerada."
      ],
      "metadata": {
        "id": "uNYY89Qflx-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***FAISS*** (*Facebook AI Similarity Search*) Ã© uma biblioteca desenvolvida pelo Facebook AI Research para realizar busca eficiente e similaridade de alta dimensÃ£o em grandes conjuntos de dados. Ã‰ projetada para lidar com grandes volumes de dados e facilitar a recuperaÃ§Ã£o de informaÃ§Ãµes baseadas em similaridade."
      ],
      "metadata": {
        "id": "XyeWNHnameDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***FAISS*** Ã© especializado em encontrar vetores (representaÃ§Ãµes numÃ©ricas de dados) similares a um vetor de consulta."
      ],
      "metadata": {
        "id": "nDV39Auimo6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Vectorstore*** Ã© um termo usado para descrever um tipo de armazenamento especializado em vetores, frequentemente utilizado em sistemas de recuperaÃ§Ã£o de informaÃ§Ãµes e modelos de aprendizado de mÃ¡quina. Ele Ã© projetado para gerenciar e buscar vetores de alta dimensÃ£o de maneira eficiente."
      ],
      "metadata": {
        "id": "XKuF39_gm0sm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bf78b11"
      },
      "source": [
        "!pip install faiss-cpu -qU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui estÃ¡ um exemplo simples que utiliza o `vectorstore` `FAISS`."
      ],
      "metadata": {
        "id": "kVWrXJiGnc8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "textos = text_splitter.split_text(documento.page_content)\n",
        "vectorstore = FAISS.from_texts(textos, embedding = OpenAIEmbeddings(api_key=OPENAI_API_KEY))\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_kwargs={\"k\": 1}\n",
        ")  # extrai apenas do primeiro documento"
      ],
      "metadata": {
        "id": "kbwZ3KU8h_uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste caso, o extrator ***RAG*** estÃ¡ analisando apenas o principal documento."
      ],
      "metadata": {
        "id": "lr3WVRPnnk4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_extrator = {\n",
        "    \"texto\": retriever | (lambda docs: docs[0].page_content)\n",
        "} | extrator"
      ],
      "metadata": {
        "id": "caWVSiHnnm5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = rag_extrator.invoke(\"Principais desenvolvimentos associados aos carros\")"
      ],
      "metadata": {
        "id": "96CqLaHrn1SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for desenvolvimento in resposta.desenvolvimentos:\n",
        "    print(desenvolvimento)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd3xtBMooLex",
        "outputId": "93f3e75b-985b-4ef5-cecf-dd8f5a40bc0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ano=1934 descricao='Avantâ€”first mass-produced front-wheel drive car, built with monocoque chassis' evidencia='Avantâ€”first mass-produced front-wheel drive car, built with monocoque chassis'\n",
            "ano=1936 descricao='InÃ­cio da produÃ§Ã£o da MG T series, uma sÃ©rie de carros esportivos que durou atÃ© 1955.' evidencia='1936â€“1955 MG T seriesâ€”sports cars'\n",
            "ano=1938 descricao='InÃ­cio da produÃ§Ã£o do Volkswagen Beetle, que se tornou um dos designs mais duradouros da histÃ³ria automotiva, com mais de 20 milhÃµes de unidades produzidas atÃ© 2003.' evidencia='1938â€“2003 Volkswagen Beetleâ€”a design that was produced for over 60 years with over 20 million units assembled in several countries'\n",
            "ano=1946 descricao='LanÃ§amento do GAZ-M20 Pobeda, um dos primeiros carros de produÃ§Ã£o em massa com design de pontÃ£o.' evidencia='1946 GAZ-M20 Pobeda, one of the first mass-produced cars with a pontoon design'\n",
            "ano=1949 descricao='IntroduÃ§Ã£o de motores V8 de alta compressÃ£o e corpos modernos pela General Motors, marcando a saÃ­da do design militar e a sombra da guerra.' evidencia=\"Automobile design and production finally emerged from the military orientation and the shadow of war in 1949, the year high-compression V8 engines and modern bodies from General Motors's Oldsmobile and Cadillac brands were introduced.\"\n",
            "ano=1955 descricao='LanÃ§amento do Chevrolet Bel Air, um modelo icÃ´nico da dÃ©cada de 1950.' evidencia='1955 Chevrolet Bel Air convertible'\n",
            "ano=1959 descricao='LanÃ§amento do Morris Mini-Minor, um carro pequeno e inovador que se tornou um Ã­cone.' evidencia='1959 Morris Mini-Minor'\n",
            "ano=1964 descricao='LanÃ§amento do Ford Mustang, que criou um novo segmento de mercado, o pony car.' evidencia='In 1964, the Ford Mustang developed a new market segment, the pony car.'\n",
            "ano=1973 descricao='A crise do petrÃ³leo de 1973 impactou significativamente a indÃºstria automotiva, levando a uma maior demanda por carros menores.' evidencia='The 1970s were turbulent years for automakers and buyers, with prominent events reshaping the industry, such as the 1973 oil crisis.'\n",
            "ano=1984 descricao='LanÃ§amento do Renault Espace, o primeiro carro de uma classe de MPV nÃ£o comercial.' evidencia='1984â€“present Renault Espaceâ€”first mass one-volume car of noncommercial MPV class.'\n",
            "ano=2023 descricao='O Tesla Model Y se torna o carro elÃ©trico mais vendido do mundo e o nome mais vendido do ano.' evidencia=\"A Tesla Model Y electric car, the world's best-selling electric car, and the best selling nameplate in 2023.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for desenvolvimento in sorted(resposta.desenvolvimentos, key=lambda x: x.ano):\n",
        "    print(f\"{desenvolvimento.ano}: {desenvolvimento.descricao}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VfP7lQFoCFb",
        "outputId": "d5734319-5254-45b5-ffae-62326f137bc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1934: Avantâ€”first mass-produced front-wheel drive car, built with monocoque chassis\n",
            "1936: InÃ­cio da produÃ§Ã£o da MG T series, uma sÃ©rie de carros esportivos que durou atÃ© 1955.\n",
            "1938: InÃ­cio da produÃ§Ã£o do Volkswagen Beetle, que se tornou um dos designs mais duradouros da histÃ³ria automotiva, com mais de 20 milhÃµes de unidades produzidas atÃ© 2003.\n",
            "1946: LanÃ§amento do GAZ-M20 Pobeda, um dos primeiros carros de produÃ§Ã£o em massa com design de pontÃ£o.\n",
            "1949: IntroduÃ§Ã£o de motores V8 de alta compressÃ£o e corpos modernos pela General Motors, marcando a saÃ­da do design militar e a sombra da guerra.\n",
            "1955: LanÃ§amento do Chevrolet Bel Air, um modelo icÃ´nico da dÃ©cada de 1950.\n",
            "1959: LanÃ§amento do Morris Mini-Minor, um carro pequeno e inovador que se tornou um Ã­cone.\n",
            "1964: LanÃ§amento do Ford Mustang, que criou um novo segmento de mercado, o pony car.\n",
            "1973: A crise do petrÃ³leo de 1973 impactou significativamente a indÃºstria automotiva, levando a uma maior demanda por carros menores.\n",
            "1984: LanÃ§amento do Renault Espace, o primeiro carro de uma classe de MPV nÃ£o comercial.\n",
            "2023: O Tesla Model Y se torna o carro elÃ©trico mais vendido do mundo e o nome mais vendido do ano.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANTE:**\n",
        "\n",
        "MÃ©todos diferentes tÃªm suas prÃ³prias vantagens e desvantagens relacionadas a custo, velocidade e precisÃ£o.\n",
        "\n",
        "Fique atento a esses problemas:\n",
        "\n",
        "- Dividir o conteÃºdo em partes significa que o LLM pode falhar em extrair informaÃ§Ãµes se essas informaÃ§Ãµes estiverem espalhadas por vÃ¡rios fragmentos.\n",
        "- Um grande sobreposiÃ§Ã£o entre os fragmentos pode fazer com que a mesma informaÃ§Ã£o seja extraÃ­da duas vezes, entÃ£o esteja preparado para desduplicar!\n",
        "- LLMs podem inventar dados. Se vocÃª estiver procurando por um Ãºnico fato em um texto longo e usar uma abordagem de forÃ§a bruta, pode acabar obtendo mais dados inventados."
      ],
      "metadata": {
        "id": "Q9pm-gJzoyeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Como usar apenas o *prompting* para realizar a extraÃ§Ã£o**"
      ],
      "metadata": {
        "id": "H6T0Amt3TRRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recursos de chamada de `ferramentas` nÃ£o sÃ£o necessÃ¡rios para gerar saÃ­das estruturadas a partir de LLMs. LLMs que conseguem seguir bem as instruÃ§Ãµes do *prompt* podem ser encarregados de fornecer informaÃ§Ãµes em um formato especÃ­fico.\n",
        ">\n",
        "Essa abordagem depende de projetar bons *prompts* e, em seguida, fazer o *parser* da saÃ­da dos LLMs para garantir que eles extraÃ­am informaÃ§Ãµes de forma eficaz.\n",
        ">\n",
        "Para extrair dados sem recursos de chamada de `ferramentas`:\n",
        ">\n",
        "1. Instrua o LLM a gerar texto seguindo um formato esperado (por exemplo, `JSON` com um determinado esquema);\n",
        "2. Use *parsers* de saÃ­da para estruturar a resposta do modelo em um objeto Python desejado."
      ],
      "metadata": {
        "id": "U3ra5F8PU4cE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nem todos os modelos suportam `.with_structured_output()`, jÃ¡ que nem todos os modelos tÃªm suporte para chamadas de `ferramentas` ou modo `JSON`. Para esses modelos, vocÃª precisarÃ¡ solicitar diretamente ao modelo que use um formato especÃ­fico e usar um *parser* de saÃ­da para extrair a resposta estruturada da saÃ­da bruta do modelo.\n"
      ],
      "metadata": {
        "id": "JwiTIHxC6go0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usando `PydanticOutputParser`**"
      ],
      "metadata": {
        "id": "EuhAtiWJ66MI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O exemplo a seguir usa o `PydanticOutputParser` embutido para fazer o *parser* da saÃ­da de um modelo de *chat* solicitado a corresponder ao esquema `Pydantic` fornecido. Note que estamos adicionando `format_instructions` diretamente ao *prompt* a partir de um mÃ©todo no *parser*:"
      ],
      "metadata": {
        "id": "shiUhUba6_xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Pessoa(BaseModel):\n",
        "    \"\"\"InformaÃ§Ã£o sobre uma pessoa.\"\"\"\n",
        "\n",
        "    nome: str = Field(..., description=\"O nome da pessoa.\")\n",
        "    altura_metros: float = Field(\n",
        "        ..., description=\"A altura da pessoa expressa em metros.\"\n",
        "    )\n",
        "\n",
        "class Pessoas(BaseModel):\n",
        "    \"\"\"InformaÃ§Ãµes de identificaÃ§Ã£o sobre todas as pessoas em um texto.\"\"\"\n",
        "\n",
        "    pessoas: List[Pessoa]\n",
        "\n",
        "# parser\n",
        "parser = PydanticOutputParser(pydantic_object=Pessoas)\n",
        "\n",
        "# prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Responda Ã  consulta do usuÃ¡rio. Envolva a saÃ­da em tags `json`\\n{format_instructions}\",\n",
        "        ),\n",
        "        (\"human\", \"{consulta}\"),\n",
        "    ]\n",
        ").partial(format_instructions = parser.get_format_instructions())"
      ],
      "metadata": {
        "id": "8iNncLbbTJB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos dar uma olhada em quais informaÃ§Ãµes serÃ£o enviadas ao modelo:"
      ],
      "metadata": {
        "id": "OMLEIvFQ-LkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "consulta = \"Ana tem 23 anos e mede 1,83 metros de altura.\"\n",
        "\n",
        "print(prompt.invoke(consulta).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw46s5HbTM23",
        "outputId": "59f9a96a-e325-42b9-b942-d39b5e1f1c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Responda Ã  consulta do usuÃ¡rio. Envolva a saÃ­da em tags `json`\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"$defs\": {\"Pessoa\": {\"description\": \"InformaÃ§Ã£o sobre uma pessoa.\", \"properties\": {\"nome\": {\"description\": \"O nome da pessoa.\", \"title\": \"Nome\", \"type\": \"string\"}, \"altura_metros\": {\"description\": \"A altura da pessoa expressa em metros.\", \"title\": \"Altura Metros\", \"type\": \"number\"}}, \"required\": [\"nome\", \"altura_metros\"], \"title\": \"Pessoa\", \"type\": \"object\"}}, \"description\": \"InformaÃ§Ãµes de identificaÃ§Ã£o sobre todas as pessoas em um texto.\", \"properties\": {\"pessoas\": {\"items\": {\"$ref\": \"#/$defs/Pessoa\"}, \"title\": \"Pessoas\", \"type\": \"array\"}}, \"required\": [\"pessoas\"]}\n",
            "```\n",
            "Human: Ana tem 23 anos e mede 1,83 metros de altura.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E agora, vamos invocÃ¡-lo:"
      ],
      "metadata": {
        "id": "hB8IhwrJ-ZlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | modelo | parser\n",
        "\n",
        "resposta = chain.invoke({\"consulta\": consulta})"
      ],
      "metadata": {
        "id": "CEsllJoPTV1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAUEHVGS9o49",
        "outputId": "8fddc03a-3f19-442f-dcc9-bbd58f81781b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pessoas(pessoas=[Pessoa(nome='Ana', altura_metros=1.83)])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "_JlQcUbfWQ0T",
        "outputId": "81a1bea9-e3c4-4c2d-ad9f-81decb685825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.Pessoas"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>Pessoas</b><br/>def __init__(self, /, **data: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>InformaÃ§Ãµes de identificaÃ§Ã£o sobre todas as pessoas em um texto.</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.pessoas[0].nome"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "prAhp-AY9h6t",
        "outputId": "d590e81f-5769-4d80-bb22-783e3d4bac2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ana'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for pessoa in resposta.pessoas:\n",
        "    print(pessoa.nome)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60oG2oihWeu_",
        "outputId": "1c9f2f2e-2ebd-4645-cf8f-ed79511dd00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ana\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Parser* Personalizado**"
      ],
      "metadata": {
        "id": "yKzFyD-j-mg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "VocÃª tambÃ©m pode criar um *prompt* e um *parser* personalizados com a **Linguagem de ExpressÃ£o LangChain** (`LCEL`), usando uma funÃ§Ã£o simples para fazer o *parser* da saÃ­da do modelo:"
      ],
      "metadata": {
        "id": "1UAHjMqp-mnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Pessoa(BaseModel):\n",
        "    \"\"\"InformaÃ§Ã£o sobre uma pessoa.\"\"\"\n",
        "\n",
        "    nome: str = Field(..., description=\"O nome da pessoa.\")\n",
        "    altura_metros: float = Field(\n",
        "        ..., description=\"A altura da pessoa expressa em metros.\"\n",
        "    )\n",
        "\n",
        "class Pessoas(BaseModel):\n",
        "    \"\"\"InformaÃ§Ãµes de identificaÃ§Ã£o sobre todas as pessoas em um texto.\"\"\"\n",
        "\n",
        "    pessoas: List[Pessoa]\n",
        "\n",
        "# prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Responda Ã  consulta do usuÃ¡rio. Saia com sua resposta como JSON que  \"\n",
        "            \"corresponda ao esquema fornecido: json\\n{schema}\\n. \"\n",
        "            \"Certifique-se de envolver a resposta em tags json e .\",\n",
        "        ),\n",
        "        (\"human\", \"{consulta}\"),\n",
        "    ]\n",
        ").partial(schema=Pessoas.model_json_schema())\n",
        "\n",
        "\n",
        "# parser personalizado\n",
        "def extrair_json(mensagem: AIMessage) -> List[dict]:\n",
        "    \"\"\"Extraia o conteÃºdo JSON de uma string em que o JSON estÃ¡ embutido entre as tags ```json e ```.\n",
        "\n",
        "    ParÃ¢metros:\n",
        "        texto (str): O texto contendo o conteÃºdo JSON.\n",
        "\n",
        "    Retorna:\n",
        "        lista: Uma lista de strings JSON extraÃ­das.\n",
        "    \"\"\"\n",
        "    texto = mensagem.content\n",
        "\n",
        "    # padrÃ£o da expressÃ£o regular para corresponder a blocos JSON\n",
        "    padrao = r\"```json(.*?)```\"\n",
        "\n",
        "    # correspondÃªncias nÃ£o sobrepostas do padrÃ£o na string\n",
        "    matches = re.findall(padrao, texto, re.DOTALL)\n",
        "\n",
        "    # retorne a lista de strings JSON correspondentes, removendo quaisquer espaÃ§os em branco no inÃ­cio ou no final\n",
        "    try:\n",
        "        return [json.loads(match.strip()) for match in matches]\n",
        "    except Exception:\n",
        "        raise ValueError(f\"Falha no parser: {mensagem}\")"
      ],
      "metadata": {
        "id": "0XoG6qcxTf-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"Ana tem 23 anos e mede 1,83 metros de altura.\"\n",
        "\n",
        "print(prompt.format_prompt(consulta = texto).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uXHH15DTjsM",
        "outputId": "2677315a-5a0a-409d-9339-0ac92e3ca808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Responda Ã  consulta do usuÃ¡rio. Saia com sua resposta como JSON que  corresponda ao esquema fornecido: json\n",
            "{'$defs': {'Pessoa': {'description': 'InformaÃ§Ã£o sobre uma pessoa.', 'properties': {'nome': {'description': 'O nome da pessoa.', 'title': 'Nome', 'type': 'string'}, 'altura_metros': {'description': 'A altura da pessoa expressa em metros.', 'title': 'Altura Metros', 'type': 'number'}}, 'required': ['nome', 'altura_metros'], 'title': 'Pessoa', 'type': 'object'}}, 'description': 'InformaÃ§Ãµes de identificaÃ§Ã£o sobre todas as pessoas em um texto.', 'properties': {'pessoas': {'items': {'$ref': '#/$defs/Pessoa'}, 'title': 'Pessoas', 'type': 'array'}}, 'required': ['pessoas'], 'title': 'Pessoas', 'type': 'object'}\n",
            ". Certifique-se de envolver a resposta em tags json e .\n",
            "Human: Ana tem 23 anos e mede 1,83 metros de altura.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | modelo | extrair_json\n",
        "\n",
        "resposta = chain.invoke({\"consulta\": texto})"
      ],
      "metadata": {
        "id": "pl-BoF91TmyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvhJGwmeB4ar",
        "outputId": "4fede547-42f3-411d-9a39-cacf132e0371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'pessoas': [{'nome': 'Ana', 'altura_metros': 1.83}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8DchUNmCpTh",
        "outputId": "2978d023-32ea-455f-8995-d1b16b5f3178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta[0]['pessoas'][0]['nome']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G9YqE9piB6NN",
        "outputId": "f6b12de2-e968-4a92-95a3-063baae11233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ana'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmQp5dPjm9eU/R6x1jeYbE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}