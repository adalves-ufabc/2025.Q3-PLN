{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2025.Q3-PLN/blob/main/2025_Q3_PLN_AULA_14_Notebook_26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2025-Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmK05FgcOzL2"
      },
      "source": [
        "## **LangChain [Dados Estruturados]**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYKEbnlNTVlR"
      },
      "outputs": [],
      "source": [
        "#@title Instalando o pacote LangChain\n",
        "!pip install langchain -q U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJvs5RTcbE64",
        "outputId": "2e86ce33-7df6-4cd1-9ab7-ac6a7add348a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3.27\n"
          ]
        }
      ],
      "source": [
        "#@title Versão do LangChain\n",
        "\n",
        "import langchain\n",
        "\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Integração com o pacote da OpenAI\n",
        "\n",
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "8klfbjqKbUpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43f0efab-84b8-49a4-a19b-618ee5a8b920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/469.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.3/469.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM5RnKClbPtQ",
        "outputId": "0c8d705e-f905-4bb3-a023-b9b735afd39f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "#@title Definindo a chave da API da OpenAI\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Definindo a chave da API da OpenAI\n",
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNNrUtWNnMbY",
        "outputId": "97f39bcd-1541-40f8-ccbf-73bba56c893b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dados Estruturados**"
      ],
      "metadata": {
        "id": "6IyxUVl4sqaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dados estruturados** são dados que são organizados e formatados de maneira sistemática, permitindo fácil acesso, análise e manipulação."
      ],
      "metadata": {
        "id": "s3YnWDZt4_Yj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequentemente é útil ter um modelo que retorne uma saída que corresponda a um esquema específico."
      ],
      "metadata": {
        "id": "qDFadJzXpfWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Método `.with_structured_output()`**"
      ],
      "metadata": {
        "id": "LTG1jw9RpqhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta é a maneira mais fácil e confiável de obter saídas estruturadas. O método `with_structured_output()` é implementado para modelos que fornecem APIs nativas para estruturar saídas, como chamadas de ferramentas/funções ou modo JSON, e utiliza essas capacidades internamente.\n",
        "\n",
        ">\n",
        "\n",
        "Este método recebe um esquema como entrada, que especifica os nomes, tipos e descrições dos atributos de saída desejados. O método retorna um `Runnable` semelhante a um modelo, exceto que, em vez de produzir strings ou Mensagens, ele produz objetos correspondentes ao esquema fornecido. O esquema pode ser especificado como uma classe `TypedDict`, um `JSON Schema` ou uma classe `Pydantic`. Se `TypedDict` ou `JSON Schema` forem usados, então um dicionário será retornado pelo `Runnable`, e se uma classe `Pydantic` for usada, então um objeto `Pydantic` será retornado.\n",
        "\n",
        ">\n",
        "\n",
        "Como exemplo, vamos fazer com que um modelo gere uma piada e separe a preparação do desfecho da piada:"
      ],
      "metadata": {
        "id": "ix6Cm19UpxIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI(model=\"gpt-4o-mini\", api_key = OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "V_YgIYt9RQH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classe `Pydantic`**\n",
        "\n",
        "Se quisermos que o modelo retorne um objeto `Pydantic`, basta passar a classe `Pydantic` desejada. A principal vantagem de usar `Pydantic` é que a saída gerada pelo modelo será validada. O `Pydantic` levantará um erro se algum campo obrigatório estiver faltando ou se algum campo for do tipo errado."
      ],
      "metadata": {
        "id": "3XPr8xwdq2o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Pydantic\n",
        "class Piada(BaseModel):\n",
        "    \"\"\"Piada para contar ao usuário.\"\"\"\n",
        "\n",
        "    introducao: str = Field(description=\"A introdução da piada\")\n",
        "    arremate: str = Field(description=\"O desfecho da piada\")\n",
        "    avaliacao: Optional[int] = Field(\n",
        "        default=None, description=\"Quão engraçada é a piada, de 1 a 10\"\n",
        "    )"
      ],
      "metadata": {
        "id": "XeU2r0ZNRub2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_estruturado = modelo.with_structured_output(Piada)\n",
        "\n",
        "modelo_estruturado.invoke(\"Me conte uma piada sobre gatos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNHCO6QZRyiO",
        "outputId": "04229e74-4e0a-4821-9e57-4814bfeefb36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Piada(introducao='Por que os gatos não conseguem jogar cartões?', arremate='Porque eles sempre ficam em cima da mesa!', avaliacao=7)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Além da estrutura da classe `Pydantic`, o nome da classe, a *docstring* e os nomes e as descrições fornecidas dos parâmetros são muito importantes. Na maioria das vezes, `with_structured_output` está usando a API de chamada de funções/ferramentas de um modelo, e você pode pensar efetivamente em todas essas informações como sendo adicionadas ao *prompt* do modelo."
      ],
      "metadata": {
        "id": "cKhTN5oEsTCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`TypedDict` ou `JSON Schema`**\n",
        "\n",
        "Se você não quiser usar `Pydantic`, explicitamente não quiser validação dos argumentos ou quiser ser capaz de transmitir as saídas do modelo, pode definir seu esquema usando uma classe `TypedDict`. Opcionalmente, podemos usar uma sintaxe especial `Annotated` suportada pelo **LangChain**, que permite especificar o valor padrão e a descrição de um campo. Observe que o valor padrão não é preenchido automaticamente se o modelo não o gerar; ele é usado apenas na definição do esquema que é passado para o modelo."
      ],
      "metadata": {
        "id": "kaxvKPWBss8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "# TypedDict\n",
        "class Piada(TypedDict):\n",
        "    \"\"\"Piada para contar ao usuário.\"\"\"\n",
        "\n",
        "    introducao: Annotated[str, ..., \"introducao\"]\n",
        "    arremate: Annotated[str, ..., \"O desfecho da piada\"]\n",
        "    avaliacao: Annotated[Optional[int], None, \"Quão engraçada é a piada, de 1 a 10\"]"
      ],
      "metadata": {
        "id": "AyWxNISbSDDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_estruturado = modelo.with_structured_output(Piada)\n",
        "\n",
        "modelo_estruturado.invoke(\"Me conte uma piada sobre gatos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfPM0qZrSHMs",
        "outputId": "030137d8-d0a1-409a-d104-fb8f110c245f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'introducao': 'Por que os gatos não gostam de computadores?',\n",
              " 'arremate': 'Porque eles ficam aterrorizados com o mouse!',\n",
              " 'avaliacao': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De maneira equivalente, podemos passar um dicionário `JSON Schema`. Isso não requer importações ou classes e torna muito claro como cada parâmetro é documentado, com o custo de ser um pouco mais verboso."
      ],
      "metadata": {
        "id": "eLoBmlzRuIq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_schema = {\n",
        "    \"title\": \"piada\",\n",
        "    \"description\": \"Piada para contar ao usuário.\",\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"introducao\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"A introdução da piada\",\n",
        "        },\n",
        "        \"arremate\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"O desfecho da piada\",\n",
        "        },\n",
        "        \"avaliacao\": {\n",
        "            \"type\": \"integer\",\n",
        "            \"description\": \"Quão engraçada é a piada, de 1 a 10\",\n",
        "            \"default\": None,\n",
        "        },\n",
        "    },\n",
        "    \"required\": [\"introducao\", \"arremate\"],\n",
        "}\n"
      ],
      "metadata": {
        "id": "6QgVXkkpud3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_estruturado = modelo.with_structured_output(json_schema)\n",
        "\n",
        "modelo_estruturado.invoke(\"Me conte uma piada sobre gatos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUpPTbuSSO6n",
        "outputId": "e9d88e6c-54d9-4765-a3ab-f6825eb473c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'introducao': 'Por que os gatos não brincam de esconde-esconde?',\n",
              " 'arremate': 'Porque eles sempre se escondem na caixa!',\n",
              " 'avaliacao': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Escolhendo entre múltiplos esquemas**"
      ],
      "metadata": {
        "id": "9_uEqzTGvBPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A maneira mais simples de permitir que o modelo escolha entre vários esquemas é criar um esquema \"pai\" que tenha um atributo do tipo `Union`:"
      ],
      "metadata": {
        "id": "1Hz00jHevIoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union\n",
        "\n",
        "# Pydantic\n",
        "\n",
        "class Piada(BaseModel):\n",
        "    \"\"\"Piada para contar ao usuário.\"\"\"\n",
        "\n",
        "    introducao: str = Field(description=\"A introdução da piada\")\n",
        "    arremate: str = Field(description=\"O desfecho da piada\")\n",
        "    avaliacao: Optional[int] = Field(\n",
        "        default=None, description=\"Quão engraçada é a piada, de 1 a 10\"\n",
        "    )\n",
        "\n",
        "class RespostaConversacional(BaseModel):\n",
        "    \"\"\"Responda de maneira conversacional. Seja gentil e prestativo.\"\"\"\n",
        "\n",
        "    resposta: str = Field(description=\"Uma resposta conversacional à pergunta do usuário\")\n",
        "\n",
        "class Resposta(BaseModel):\n",
        "    saida: Union[Piada, RespostaConversacional]"
      ],
      "metadata": {
        "id": "4It9eSpCSXT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_estruturado = modelo.with_structured_output(Resposta)\n",
        "\n",
        "resposta = modelo_estruturado.invoke(\"Me conte uma piada sobre gatos\")"
      ],
      "metadata": {
        "id": "YyQZ9rEFSaTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX4wPoOwwWnp",
        "outputId": "2fec46ea-de1b-4802-8989-308c3ea9a048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Resposta(saida=Piada(introducao='Por que os gatos sempre ganham em videogames?', arremate='Porque eles têm nove vidas!', avaliacao=7))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.saida"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-nV0FhDwdJq",
        "outputId": "411a5bbe-12c2-430f-b13f-d11eb340e22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Piada(introducao='Por que os gatos sempre ganham em videogames?', arremate='Porque eles têm nove vidas!', avaliacao=7)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.saida.introducao"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "w5SWiT88wiqg",
        "outputId": "472b4ffe-946c-4d59-b0ba-b3e55a41db9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Por que os gatos sempre ganham em videogames?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_estruturado.invoke(\"Como você está hoje?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-J3wN4QSfW2",
        "outputId": "b88933a1-6985-47c8-eca9-c50e6821d37a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Resposta(saida=RespostaConversacional(resposta='Estou ótimo, obrigado por perguntar! E você, como está hoje?'))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Few-shot prompting***"
      ],
      "metadata": {
        "id": "iozPUz_PyUnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Few-shot prompting*** é uma técnica onde se fornecem alguns exemplos específicos no *prompt* para guiar o modelo na geração de respostas. Em vez de treinar o modelo com uma nova base de dados completa, você apenas inclui alguns exemplos relevantes diretamente no *prompt*."
      ],
      "metadata": {
        "id": "-lA-SqH2yz_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para esquemas mais complexos, é muito útil adicionar exemplos ao *prompt*.\n",
        "\n",
        "A maneira mais simples e universal é adicionar exemplos a uma mensagem do sistema no *prompt*:"
      ],
      "metadata": {
        "id": "x2SVsWbDyMit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "sistema = \"\"\"Você é um comediante hilário. Sua especialidade são piadas do tipo \"knock-knock\".\n",
        "Retorne uma piada que tenha a introdução (a resposta para \"Quem está aí?\") e o arremate final (a resposta para \"<introdução> quem?\").\n",
        "\n",
        "Aqui estão alguns exemplos de piadas:\n",
        "\n",
        "exemplo_usuario: Me conte uma piada sobre gatos\n",
        "exemplo_assistente: {{\"introducao\": \"Por que os gatos não jogam cartas?\", \"arremate\": \"Porque eles têm medo dos cães que podem trapacear!\", \"avaliacao\": 8}}\n",
        "\n",
        "exemplo_usuario: Conte outra piada sobre cães\n",
        "exemplo_assistente: {{\"introducao\": \"Por que os cães levam uma bola para a escola?\", \"arremate\": \"Porque eles querem aprender a jogar!\", \"ratavaliacaoing\": 7}}\n",
        "\n",
        "exemplo_usuario: Agora sobre peixes\n",
        "exemplo_assistente: {{\"introducao\": \"Por que o peixe foi ao banco?\", \"arremate\": \"Para abrir uma conta corrente!\", \"avaliacao\": 9}}\"\"\"\n",
        "\n",
        "# template de prompt\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", sistema), (\"human\", \"{input}\")])\n",
        "\n",
        "# uso do prompt com um modelo estruturado\n",
        "modelo_estruturado_few_shot = prompt | modelo_estruturado\n",
        "\n",
        "modelo_estruturado_few_shot.invoke(\"Me conte uma piada sobre elefantes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4rFakRh0kYC",
        "outputId": "064f23ad-021b-4854-dc76-2b7fc98b17a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Resposta(saida=Piada(introducao='Por que os elefantes nunca usam computador?', arremate='Porque eles têm medo do mouse!', avaliacao=7))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outro exemplo:"
      ],
      "metadata": {
        "id": "7ee9wq0Y2P4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pydantic\n",
        "\n",
        "class Produto(BaseModel):\n",
        "    \"\"\"Descrição de um produto.\"\"\"\n",
        "\n",
        "    nome: str = Field(description=\"O nome do produto\")\n",
        "    descricao: str = Field(description=\"A descrição do produto\")"
      ],
      "metadata": {
        "id": "QMMaTjab28rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_estruturado = modelo.with_structured_output(Produto)"
      ],
      "metadata": {
        "id": "rQpTkm6F3LQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "sistema = \"\"\"Você é um redator de descrições de produtos experiente. Sua tarefa é criar descrições detalhadas e envolventes para produtos.\n",
        "Cada descrição deve incluir informações sobre as principais características e benefícios do produto.\n",
        "\n",
        "Aqui estão alguns exemplos de descrições:\n",
        "\n",
        "exemplo_usuario: Descreva um smartphone moderno\n",
        "exemplo_assistente: {{\"nome\": \"Smartphone UltraX\", \"descricao\": \"O Smartphone UltraX é equipado com uma tela AMOLED de 6,7 polegadas, processador octa-core de última geração e câmera tripla de 64MP. Oferece um desempenho rápido e uma experiência de usuário excepcional com bateria de longa duração e carregamento rápido.\"}}\n",
        "\n",
        "exemplo_usuario: Descreva um aspirador de pó robô\n",
        "exemplo_assistente: {{\"nome\": \"Aspirador Robô CleanPro\", \"descricao\": \"O Aspirador Robô CleanPro é ideal para manter sua casa limpa sem esforço. Com tecnologia de navegação inteligente, ele limpa todos os cantos e vem com um sistema de filtragem HEPA que captura alérgenos e poeira. Compacto e silencioso, é fácil de usar e programar.\"}}\n",
        "\n",
        "exemplo_usuario: Agora sobre uma cafeteira\n",
        "exemplo_assistente: {{\"nome\": \"Cafeteira Espresso Elite\", \"descricao\": \"A Cafeteira Espresso Elite oferece uma experiência de café de qualidade barista no conforto de sua casa. Com um sistema de pressão de 15 bar, ela extrai o máximo sabor dos grãos. Possui um moinho integrado e controle de temperatura preciso para preparar o café perfeito a cada vez.\"}}\"\"\"\n",
        "\n",
        "# template de prompt\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", sistema), (\"human\", \"{input}\")])\n",
        "\n",
        "# uso do prompt com um modelo estruturado\n",
        "modelo_estruturado_few_shot = prompt | modelo_estruturado\n",
        "\n",
        "modelo_estruturado_few_shot.invoke(\"Descreva um tablet moderno\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KlnXwtc2RSd",
        "outputId": "7489eb56-8e75-49b1-f988-dba051506f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Produto(nome='Tablet ProTech 12', descricao='O Tablet ProTech 12 é a combinação perfeita de potência e portabilidade. Com uma tela Retina de 12,9 polegadas, suas cores vibrantes e detalhes nítidos tornam a visualização de vídeos e trabalho em aplicativos uma experiência envolvente. Equipado com um processador Snapdragon de última geração, ele oferece desempenho rápido em multitarefas e jogos exigentes. A bateria de longa duração garante que você possa trabalhar ou se divertir ao longo do dia sem se preocupar em recarregar. Além disso, a caneta stylus incluída permite uma escrita e desenho mais precisos, ideal para profissionais criativos.')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para modelos que suportam mais de um meio de estruturar saídas, você pode especificar qual método usar com o argumento `method=`."
      ],
      "metadata": {
        "id": "wkt2Pm_p5SSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_estruturado = modelo.with_structured_output(None, method=\"json_mode\")\n",
        "\n",
        "modelo_estruturado.invoke(\n",
        "    \"Descreva um tablet moderno, responda em JSON com as chaves `nome` e `descricao`\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6mOsAWuS43A",
        "outputId": "02d36bf5-8995-46b8-81bb-40ecf17ff57d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nome': 'Tablet X Pro 2023',\n",
              " 'descricao': 'O Tablet X Pro 2023 é um dispositivo premium com tela de 12,9 polegadas Retina Liquid, oferecendo cores vibrantes e excelente nitidez. Equipado com um processador ultra-rápido, 8 GB de RAM e opções de armazenamento de até 512 GB, ele é perfeito para multitarefa e desempenho em aplicativos gráficos. Possui suporte para caneta stylus, permitindo desenho e anotações precisas, e uma bateria que dura até 15 horas. Conectividade Wi-Fi 6 e 5G garantem velocidade de internet, enquanto um design leve e elegante com molduras finas proporciona portabilidade e estilo. O tablet também vem com sistema de som surround estéreo e câmeras de alta resolução para fotos e videochamadas.'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelos de linguagem não são perfeitos ao gerar saídas estruturadas, especialmente quando os esquemas se tornam complexos. Você pode evitar a geração de exceções e lidar com a saída bruta você mesmo passando `include_raw=True`. Isso altera o formato da saída para conter a mensagem bruta, o valor analisado (se bem-sucedido) e quaisquer erros resultantes."
      ],
      "metadata": {
        "id": "1h3UqxgM6Ch3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_estruturado = modelo.with_structured_output(Produto, include_raw=True)\n",
        "\n",
        "modelo_estruturado.invoke(\"Descreva um tablet moderno\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXuncLbSS_dw",
        "outputId": "64dadec7-68f1-466d-a394-22b56f310878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'raw': AIMessage(content='{\"nome\":\"Tablet Ultra Slim 2023\",\"descricao\":\"O Tablet Ultra Slim 2023 é um dispositivo leve e portátil, com um design elegante e moderno. Possui uma tela de 12,9 polegadas com tecnologia AMOLED, oferecendo cores vibrantes e alta definição, ideal para assistir filmes e jogar. Equipado com um processador de última geração e 8 GB de RAM, garante desempenho fluido em todas as tarefas, desde trabalhos de escritório até jogos pesados. A bateria de longa duração suporta até 12 horas de uso contínuo, e a conectividade inclui Wi-Fi 6, Bluetooth 5.1 e suporte a redes 5G. Com sistema operacional Android 13, o tablet oferece uma vasta gama de aplicativos e uma interface intuitiva. Além disso, sua câmera traseira de 12 MP e frontal de 8 MP permitem tirar fotos e realizar chamadas de vídeo com qualidade excepcional.\"}', additional_kwargs={'parsed': Produto(nome='Tablet Ultra Slim 2023', descricao='O Tablet Ultra Slim 2023 é um dispositivo leve e portátil, com um design elegante e moderno. Possui uma tela de 12,9 polegadas com tecnologia AMOLED, oferecendo cores vibrantes e alta definição, ideal para assistir filmes e jogar. Equipado com um processador de última geração e 8 GB de RAM, garante desempenho fluido em todas as tarefas, desde trabalhos de escritório até jogos pesados. A bateria de longa duração suporta até 12 horas de uso contínuo, e a conectividade inclui Wi-Fi 6, Bluetooth 5.1 e suporte a redes 5G. Com sistema operacional Android 13, o tablet oferece uma vasta gama de aplicativos e uma interface intuitiva. Além disso, sua câmera traseira de 12 MP e frontal de 8 MP permitem tirar fotos e realizar chamadas de vídeo com qualidade excepcional.'), 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 186, 'prompt_tokens': 101, 'total_tokens': 287, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CXWCwXufzdSuBctwYlsGGcxNgVaG4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--c69c3d53-0135-4d88-8b1e-bd76560fdbcb-0', usage_metadata={'input_tokens': 101, 'output_tokens': 186, 'total_tokens': 287, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " 'parsed': Produto(nome='Tablet Ultra Slim 2023', descricao='O Tablet Ultra Slim 2023 é um dispositivo leve e portátil, com um design elegante e moderno. Possui uma tela de 12,9 polegadas com tecnologia AMOLED, oferecendo cores vibrantes e alta definição, ideal para assistir filmes e jogar. Equipado com um processador de última geração e 8 GB de RAM, garante desempenho fluido em todas as tarefas, desde trabalhos de escritório até jogos pesados. A bateria de longa duração suporta até 12 horas de uso contínuo, e a conectividade inclui Wi-Fi 6, Bluetooth 5.1 e suporte a redes 5G. Com sistema operacional Android 13, o tablet oferece uma vasta gama de aplicativos e uma interface intuitiva. Além disso, sua câmera traseira de 12 MP e frontal de 8 MP permitem tirar fotos e realizar chamadas de vídeo com qualidade excepcional.'),\n",
              " 'parsing_error': None}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNp1J1DEXvPdzZyNm5FDSiE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}