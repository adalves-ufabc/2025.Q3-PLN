{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2025.Q3-PLN/blob/main/2025_Q3_PLN_AULA_13_Notebook_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2025-Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmK05FgcOzL2"
      },
      "source": [
        "## **LangChain**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYKEbnlNTVlR"
      },
      "outputs": [],
      "source": [
        "#@title Instalando o pacote LangChain\n",
        "!pip install langchain -q U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJvs5RTcbE64",
        "outputId": "f8b1e590-5e6f-4442-c3fd-03384553e9c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3.27\n"
          ]
        }
      ],
      "source": [
        "#@title Versão do LangChain\n",
        "\n",
        "import langchain\n",
        "\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Integração com o pacote da OpenAI\n",
        "\n",
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "8klfbjqKbUpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d89f773-fe35-4e5e-af8e-0f1909779c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/467.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m460.8/467.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.1/467.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM5RnKClbPtQ",
        "outputId": "50c6b709-17f8-4f92-ea10-b3c9fd553aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "#@title Definindo a chave da API da OpenAI\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5GGZ0e66kma"
      },
      "source": [
        "## **Cadeia (*Chain*)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No **LangChain**, uma **cadeia** é uma sequência de operações que conecta vários componentes, como modelos de linguagem, *parsers* de saída (um componente que processa a saída do modelo para formatação) e transformações de dados, para criar um fluxo de trabalho coeso. A **cadeia** permite combinar diferentes passos de processamento e manipulação de dados de maneira estruturada e eficiente.\n",
        "\n",
        "As **cadeias** são configuradas para permitir um fluxo contínuo de dados entre os componentes, facilitando a construção de aplicações complexas de PLN."
      ],
      "metadata": {
        "id": "P00KwFs3EtUn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ygo9oZC8qgV"
      },
      "source": [
        "Podemos então inicializar o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QUTKqod4en0"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI( temperature = 0.9, max_tokens= 50 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zflVvVO_83Nj"
      },
      "source": [
        "Depois de instalar e inicializar o modelo de sua escolha, podemos tentar usá-lo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8CL95XV4whj",
        "outputId": "c22a1e7b-efb9-468d-9025-55153b505c13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Arco-Íris Meias', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 38, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CVz2ZtKwwWEsL2N3L5jtfpg16FW5r', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--ac57494f-14dc-463d-87dd-561517e18dbb-0', usage_metadata={'input_tokens': 38, 'output_tokens': 7, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "prompt = \"Qual seria um bom nome para uma empresa que fabrica meias coloridas? Responda em Português e retorne apenas o nome da empresa\"\n",
        "\n",
        "modelo.invoke(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyD8bN8kr33V"
      },
      "source": [
        "**Templates de *prompt***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO_pNKVp9sFt"
      },
      "source": [
        "Também podemos orientar a resposta com um template de *prompt*. Os templates de *prompt* são usados para converter a entrada bruta do usuário em uma entrada melhor para o modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`PromptTemplate`**"
      ],
      "metadata": {
        "id": "9wqgDjAkF2X5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D83NtUG90Sx"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables = [\"produto\"],\n",
        "    template = \"Qual seria um bom nome para uma empresa que fabrica {produto}? Responda em Português e retorne apenas o nome da empresa?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwq9WW61-QJd"
      },
      "source": [
        "Agora podemos combiná-los em uma cadeia simples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uyqY35m-A4C"
      },
      "outputs": [],
      "source": [
        "chain = prompt | modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tre_BeCW-HPH",
        "outputId": "5495eaf1-6d55-4864-910f-386e2de0e75d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Fornalizza.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 36, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CVz2iM3VYZ6UT1EBYD0EiXcl1Uj3g', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--2b38971d-d2ed-4e8f-9b48-e06df1ac5fde-0', usage_metadata={'input_tokens': 36, 'output_tokens': 5, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "chain.invoke({\"produto\": \"pizza\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`ChatPromptTemplate`**"
      ],
      "metadata": {
        "id": "IN3LCIgeFvVb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgMoHyBhIWMN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Você é um estrategista de marketing digital especializado em {setor}\"),\n",
        "    (\"user\", \"Considerando a crescente digitalização no {setor}, proponha um slogan cativante para uma campanha publicitária.\")\n",
        "])\n",
        "\n",
        "chain = prompt | modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Gzm3PzNJKbQ",
        "outputId": "fa25e54c-1582-43fb-927f-0827de7bfff2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='\"Contabilidade digital: simplificando e modernizando sua empresa!\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CVz2plAio0EYtXymyo9rpae9kaPx0', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--d31f7152-2cca-49eb-9c4f-06190377cf6a-0', usage_metadata={'input_tokens': 55, 'output_tokens': 14, 'total_tokens': 69, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "chain.invoke({\"setor\": \"contábil\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`MessagesPlaceholder`**"
      ],
      "metadata": {
        "id": "gCtPz2LhHhs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este modelo de *prompt* é responsável por adicionar uma lista de mensagens em um determinado local.\n",
        "\n",
        " `MessagesPlaceholder` é uma classe usada no **LangChain** para representar um espaço reservado dentro de um modelo de *prompt*, onde mensagens dinâmicas podem ser inseridas posteriormente. Ele atua como um marcador que indica onde as mensagens devem ser incluídas na estrutura do *prompt*."
      ],
      "metadata": {
        "id": "PfpGORbdHkPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Define o template de prompt com uma mensagem do sistema e um espaço reservado para mensagens do usuário\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Você é um assistente de viagem experiente. Ajude o usuário a planejar uma viagem.\"),\n",
        "    MessagesPlaceholder(\"usuario_msgs\")\n",
        "])\n",
        "\n",
        "# Cria um dicionário com mensagens do usuário\n",
        "mensagens_usuario = [\n",
        "    HumanMessage(content=\"Estou planejando uma viagem para a Europa. Quais são as melhores cidades para visitar?\"),\n",
        "    HumanMessage(content=\"Pode me dar sugestões de atividades em Paris?\"),\n",
        "    HumanMessage(content=\"Responda as duas perguntas anteriores\")\n",
        "]\n",
        "\n",
        "# Invoca o template com as mensagens do usuário\n",
        "resultado = prompt_template.invoke({\"usuario_msgs\": mensagens_usuario})"
      ],
      "metadata": {
        "id": "fwRSRfWCr2SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultado"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TziWWYSsmbw",
        "outputId": "0b007cfb-3cf3-4fca-dad0-710510644c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Você é um assistente de viagem experiente. Ajude o usuário a planejar uma viagem.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Estou planejando uma viagem para a Europa. Quais são as melhores cidades para visitar?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Pode me dar sugestões de atividades em Paris?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Responda as duas perguntas anteriores', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultado.messages[0].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CPTj20Grsch7",
        "outputId": "209f5fad-0e8b-47ed-9f41-b89227fc4936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Você é um assistente de viagem experiente. Ajude o usuário a planejar uma viagem.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrando com um modelo de linguagem:"
      ],
      "metadata": {
        "id": "4IFrzt7ZzOA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI( model=\"gpt-4o\", temperature = 0.9, max_tokens= 1024 )"
      ],
      "metadata": {
        "id": "Hzh-YuKuzqMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | modelo"
      ],
      "metadata": {
        "id": "wRLfxQvRyR3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chain.invoke({\"usuario_msgs\": mensagens_usuario})"
      ],
      "metadata": {
        "id": "_MU1J192yZ4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(resposta.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "jScr9Bk6zfuY",
        "outputId": "eae63c10-a1c5-472a-cc76-091af23af724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Claro, vamos lá:\n\n### Melhores Cidades para Visitar na Europa\n\n1. **Paris, França**: Conhecida como a \"Cidade da Luz\", Paris é um destino clássico com sua rica história, cultura e culinária incrível.\n   \n2. **Roma, Itália**: Com seu impressionante legado histórico e arquitetônico, Roma oferece locais como o Coliseu, o Vaticano e a Fontana di Trevi.\n   \n3. **Barcelona, Espanha**: Famosa pela arquitetura única de Gaudí, em especial a Sagrada Família, e suas praias vibrantes.\n   \n4. **Amsterdã, Países Baixos**: Conhecida por seus canais pitorescos, museus, como o de Van Gogh, e seu ambiente acolhedor.\n   \n5. **Londres, Reino Unido**: Oferece uma mistura de história antiga (como a Torre de Londres) e modernidade (como o Tate Modern).\n   \n6. **Praga, República Tcheca**: Com seu centro histórico bem preservado e o famoso Castelo de Praga, é uma cidade que parece saída de um conto de fadas.\n   \n7. **Viena, Áustria**: Rica em música clássica e arquitetura imperial, com locais como o Palácio de Schönbrunn.\n   \n8. **Lisboa, Portugal**: Conhecida por seus charmosos bairros, como Alfama, e uma culinária deliciosa.\n   \n9. **Budapeste, Hungria**: Com seus belíssimos banhos termais e a impressionante arquitetura ao longo do Danúbio.\n   \n10. **Florença, Itália**: O berço do Renascimento, com obras de arte e arquitetura incomparáveis.\n\n### Sugestões de Atividades em Paris\n\n1. **Torre Eiffel**: Suba até o topo para vistas deslumbrantes da cidade.\n   \n2. **Museu do Louvre**: Explore uma das maiores coleções de arte do mundo, incluindo a Mona Lisa.\n   \n3. **Catedral de Notre-Dame**: Visite esta magnífica catedral gótica (verifique a disponibilidade por conta da restauração).\n   \n4. **Caminhada pelos Campos Elísios**: Termine com uma visita ao Arco do Triunfo.\n   \n5. **Museu d'Orsay**: Conhecido por suas coleções impressionistas, localiza-se em uma antiga estação de trem.\n   \n6. **Montmartre e a Basílica de Sacré-Cœur**: Passeie por este bairro boêmio e desfrute das vistas do alto da colina.\n   \n7. **Passeio de Barco no Sena**: Veja os principais pontos turísticos de uma perspectiva diferente.\n   \n8. **Palácio de Versalhes**: Fazer uma viagem até Versalhes para explorar o opulento palácio e seus jardins.\n   \n9. **Jardim de Luxemburgo**: Desfrute de um dia relaxante neste lindo parque.\n   \n10. **Shows de Cabaré no Moulin Rouge**: Experimente a famosa vida noturna de Paris.\n\nEssas sugestões oferecerão uma visão abrangente da rica experiência que a Europa e Paris, especificamente, têm a oferecer. Boa viagem!"
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LangChain Expression Language (LCEL)**"
      ],
      "metadata": {
        "id": "mekuFSiI6sfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A ***LangChain Expression Language*** é uma forma de criar cadeias personalizadas e arbitrárias. Ela é construída sobre o protocolo `Runnable`.\n",
        "\n",
        "O protocolo `Runnable` do **LangChain** é uma interface que define como diferentes componentes podem ser encadeados e executados dentro de um fluxo de trabalho."
      ],
      "metadata": {
        "id": "dfNU7xsc6qPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Como encadear *runnables***"
      ],
      "metadata": {
        "id": "xuKCjEcU6_-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um ponto sobre a **LCEL** é que qualquer dois *runnables* podem ser 'encadeados' juntos em sequências. A saída da chamada `.invoke()` do *runnable* anterior é passada como entrada para o próximo *runnable*. Isso pode ser feito usando o operador pipe (`|`), ou o método mais explícito `.pipe()`, que faz a mesma coisa.\n",
        "\n",
        "A *RunnableSequence* resultante é em si mesma um *runnable*, o que significa que pode ser invocada, transmitida ou encadeada ainda mais, assim como qualquer outro *runnable*."
      ],
      "metadata": {
        "id": "tU-1g2OR73th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para demonstrar como isso funciona, vamos apresentar um exemplo:"
      ],
      "metadata": {
        "id": "FXPSz2M58QpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=1.0)"
      ],
      "metadata": {
        "id": "CMQbYqFx8nRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"me conte uma piada sobre {assunto}\")\n",
        "\n",
        "chain = prompt | modelo | StrOutputParser()"
      ],
      "metadata": {
        "id": "S-8oXgR88yIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TMbh8lGc9HYb",
        "outputId": "1fcb252d-faeb-4f4f-8732-395e78f5d3b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Por que o papagaio foi ao médico?\\nPorque estava com uma griPAPAGAIO!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coerção**"
      ],
      "metadata": {
        "id": "45bYkgVD9mti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos até combinar essa cadeia com mais *runnables* para criar outra cadeia. Isso pode envolver algum formato de entrada/saída usando outros tipos de *runnables*, dependendo dos requisitos de entrada e saída dos componentes da cadeia.\n",
        "\n",
        "Por exemplo, digamos que queremos compor a cadeia de geração de piadas com outra cadeia que avalia se a piada gerada foi engraçada ou não."
      ],
      "metadata": {
        "id": "vcoKX4xi9obx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt_analise = ChatPromptTemplate.from_template(\"essa piada é engraçada? {piada}\")\n",
        "\n",
        "chain_composto = {\"piada\": chain} | prompt_analise | modelo | StrOutputParser()\n",
        "\n",
        "chain_composto.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UqxEG7U_9-MY",
        "outputId": "f2576830-a7a1-4852-efed-ef867a9b5608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sim, essa piada é engraçada porque brinca com a ideia de um papagaio falante causando confusão em uma escola, sendo expulso por falar muitas asneiras durante a aula. É uma situação inusitada e engraçada.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No contexto do **LangChain**, **coerção** refere-se ao processo de converter automaticamente ou adaptar um tipo de dado para outro, de modo que ele se encaixe nos requisitos de entrada ou saída de uma cadeia ou componente específico. Isso pode acontecer, por exemplo, quando você passa dados entre diferentes *runnables* (componentes que seguem o protocolo `Runnable`) em uma cadeia, e o **LangChain** ajusta automaticamente os dados para que sejam compatíveis com o próximo componente.\n",
        "\n",
        "Por exemplo, se você passar um dicionário como entrada em uma cadeia que espera um formato específico, o **LangChain** pode coagir (converter) automaticamente esse dicionário para o formato adequado, como um `RunnableParallel`, que executa operações em paralelo.\n",
        "\n",
        "Esse mecanismo de coerção facilita a construção de cadeias complexas, pois minimiza a necessidade de manipulação manual dos dados para garantir que sejam compatíveis com cada etapa do processo."
      ],
      "metadata": {
        "id": "yREWWO16-k4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**O método `.pipe()`**"
      ],
      "metadata": {
        "id": "XGd0uDIF_cr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "chain_composto_pipe = (\n",
        "    RunnableParallel({\"piada\": chain})\n",
        "    .pipe(prompt_analise)\n",
        "    .pipe(modelo)\n",
        "    .pipe(StrOutputParser())\n",
        ")\n",
        "\n",
        "chain_composto_pipe.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "kMsbmik__iNs",
        "outputId": "be581b8b-e2ea-46cb-9a4e-1633f7cbf2a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sim, é engraçada porque faz um trocadilho com a palavra \"poleiro\", que é onde o papagaio costuma ficar, e a expressão \"se sentindo mal\", sugerindo que o papagaio estava se sentindo mal por estar se comportando como um poleiro.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ou de forma abreviada:"
      ],
      "metadata": {
        "id": "_EnowCmEBbu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain_composto_pipe = RunnableParallel({\"piada\": chain}).pipe(\n",
        "    prompt_analise, modelo, StrOutputParser()\n",
        ")\n",
        "\n",
        "chain_composto_pipe.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u2cOvxLaBWhT",
        "outputId": "4196db30-56e3-4df0-e09d-7f903c2f74d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sim, essa piada é engraçada porque faz um trocadilho com a palavra \"rouco\", que pode significar a voz rouca de alguém e também a rouquidão de um papagaio.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Função de Depuração**\n",
        "\n",
        "Uma função de depuração é uma função usada para inspecionar, verificar ou diagnosticar o estado de um programa durante sua execução."
      ],
      "metadata": {
        "id": "jmF_bxQKA99l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def funcao_debug(output):\n",
        "    print(\"Piada gerada:\", output[\"piada\"])\n",
        "    return output\n",
        "\n",
        "chain_composto_pipe = (\n",
        "    RunnableParallel({\"piada\": chain})\n",
        "    .pipe(funcao_debug)  # adiciona uma função de depuração\n",
        "    .pipe(prompt_analise)\n",
        "    .pipe(modelo)\n",
        "    .pipe(StrOutputParser())\n",
        ")\n",
        "\n",
        "chain_composto_pipe.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "H3VZzwS4Ag0K",
        "outputId": "b4db9be8-83c3-43df-a830-763aa4430641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Piada gerada: Por que o papagaio não gostava de piadas?\n",
            "\n",
            "Porque ele achava que todas as piadas eram \"papagaiadas\"!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sim, essa piada é engraçada porque brinca com a ideia de um papagaio achando que todas as piadas são \"papagaiadas\", ou seja, coisas bobas ou tolas. É um trocadilho simples, mas eficaz.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paralelizar etapas**\n",
        "\n",
        "`RunnableParallels` facilitam a execução de múltiplos *runnables* em paralelo e o retorno da saída desses *runnables* como um mapa."
      ],
      "metadata": {
        "id": "M2eoWy8DF2Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI()\n",
        "\n",
        "chain_piada = ChatPromptTemplate.from_template(\"me conte uma piada sobre {assunto}\") | modelo\n",
        "chain_poema = ChatPromptTemplate.from_template(\"escreva um poema de 2 linhas sobre {assunto}\") | modelo\n",
        "\n",
        "chain_mapa = RunnableParallel(piada = chain_piada, poema = chain_poema)\n",
        "\n",
        "resposta = chain_mapa.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "id": "k8ZMAbJZF_Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUb0eATRG3XB",
        "outputId": "e3605905-21cd-4324-a0e9-10d9508e4ac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'piada': AIMessage(content='Por que o papagaio foi expulso da aula de matemática? Porque ele passava o dia todo fazendo conta de mentiroso!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 16, 'total_tokens': 50, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CVz4mCD1mY2w49tnyZb24W3r3UJQv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--291cac5d-d417-417f-bc4a-1a73af27b5e2-0', usage_metadata={'input_tokens': 16, 'output_tokens': 34, 'total_tokens': 50, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              " 'poema': AIMessage(content='Papagaio colorido,\\nvoz alegre no ar divertido.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 22, 'total_tokens': 39, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CVz4mQ8uBXwTmDMrj28XrOA8ZWXTY', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--ff8d5704-d385-484b-ad3b-6ba5c8447ade-0', usage_metadata={'input_tokens': 22, 'output_tokens': 17, 'total_tokens': 39, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yx_9fk7G52N",
        "outputId": "4e4e9201-5b50-492a-f73b-56227d91639a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['piada', 'poema'])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta[\"piada\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re---zkfG873",
        "outputId": "36d09287-ad69-4556-956b-d64983e2d36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Por que o papagaio foi expulso da aula de matemática? Porque ele passava o dia todo fazendo conta de mentiroso!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 16, 'total_tokens': 50, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CVz4mCD1mY2w49tnyZb24W3r3UJQv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--291cac5d-d417-417f-bc4a-1a73af27b5e2-0', usage_metadata={'input_tokens': 16, 'output_tokens': 34, 'total_tokens': 50, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta[\"piada\"].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "exDMS4eGHesN",
        "outputId": "85c65c4f-aaa2-4552-c215-f05b158ed07a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Por que o papagaio foi expulso da aula de matemática? Porque ele passava o dia todo fazendo conta de mentiroso!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`RunnableParallel` também é útil para executar processos independentes em paralelo, uma vez que cada *Runnable* no mapa é executado em paralelo. Por exemplo, podemos ver que nossas cadeias anteriores `chain_piada`, `chain_poema` e `chain_mapa` têm tempos de execução semelhantes, mesmo que `chain_mapa` execute as duas outras cadeias."
      ],
      "metadata": {
        "id": "dTyvqKMGHtbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "chain_piada.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o48wUrRdH8JA",
        "outputId": "526c472a-ff49-450d-9e18-9acc28d3bb3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "551 ms ± 83 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "chain_poema.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8uXRCn5IF7o",
        "outputId": "e84e5dd1-be73-4049-b308-908a9178cdb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "541 ms ± 120 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "\n",
        "chain_mapa.invoke({\"assunto\": \"papagaio\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6PWXxaVINjo",
        "outputId": "2615af8e-1eb8-49e5-daf0-7e6815704121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "692 ms ± 130 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+Oej+sDzBTEhECMyIOKNm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}