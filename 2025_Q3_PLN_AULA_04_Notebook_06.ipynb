{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2025.Q3-PLN/blob/main/2025_Q3_PLN_AULA_04_Notebook_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2025-Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXNV4odoXftj"
      },
      "source": [
        "## **Corre√ß√£o Ortogogr√°fica**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4hlX1pbYs-K"
      },
      "source": [
        "A corre√ß√£o ortogr√°fica em PLN √© importante por v√°rios motivos:\n",
        "\n",
        "  * Melhorar a precis√£o da an√°lise de texto: a corre√ß√£o ortogr√°fica ajuda a evitar erros de interpreta√ß√£o causados por palavras escritas incorretamente. Isso √© especialmente importante para tarefas de an√°lise de texto, como reconhecimento de entidades nomeadas, an√°lise de sentimento e classifica√ß√£o de texto.\n",
        "\n",
        "  * Aumentar a efici√™ncia da an√°lise: a corre√ß√£o ortogr√°fica permite que os algoritmos de PLN processem o texto com mais efici√™ncia, reduzindo o n√∫mero de varia√ß√µes de palavras que precisam ser consideradas.\n",
        "\n",
        "  * Melhorar a experi√™ncia do usu√°rio: corrigir erros de ortografia em texto pode melhorar a compreens√£o do leitor e a credibilidade do autor. Isso √© especialmente importante em aplica√ß√µes de PLN voltadas para usu√°rios finais, como assistentes virtuais e aplicativos de mensagens.\n",
        "\n",
        "  * Padronizar os dados de texto: a corre√ß√£o ortogr√°fica pode ajudar a padronizar a ortografia de palavras em um conjunto de dados de texto, tornando-o mais f√°cil de analisar e comparar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRPrtVYUXupO"
      },
      "source": [
        "Aqui est√° um exemplo de corre√ß√£o ortogr√°fica em portugu√™s usando a biblioteca `pyspellchecker` em `Python`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT4dPlLhXDox",
        "outputId": "e511e448-b205-4f6b-97c0-fe38cc68d6ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.3-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS3hc3AmXAop"
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "texto = \"Eu gosto de caminhar pela praja\"\n",
        "\n",
        "corretor = SpellChecker(language='pt')\n",
        "palavras = texto.split()\n",
        "\n",
        "palavras_corrigidas = []\n",
        "for palavra in palavras:\n",
        "    palavra_corrigida = corretor.correction(palavra)\n",
        "    palavras_corrigidas.append(palavra_corrigida)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLpAGC16Lt0y",
        "outputId": "153f9bd6-545b-4035-85cd-d432a1018078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Eu', 'gosto', 'de', 'caminhar', 'pela', 'praia']\n"
          ]
        }
      ],
      "source": [
        "print(palavras_corrigidas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEChCb55Lsxi",
        "outputId": "ca0dd33d-700b-419a-958d-70f7b8bd8842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu gosto de caminhar pela praia\n"
          ]
        }
      ],
      "source": [
        "texto_corrigido = \" \".join(palavras_corrigidas)\n",
        "\n",
        "print(texto_corrigido)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noOiTUXNX55O"
      },
      "source": [
        "Nesse exemplo, foi definida uma frase com uma palavra escrita incorretamente (`praja` em vez de `praia`). Em seguida, a biblioteca `pyspellchecker` √© usada para corrigir a ortografia da palavra.\n",
        "\n",
        "Para isso, criamos um objeto `SpellChecker` e o usamos para verificar a ortografia de cada palavra na frase usando o m√©todo `correction()`. Em seguida, adicionamos as palavras corrigidas a uma nova lista e, por fim, unimos as palavras em uma nova frase corrigida usando a fun√ß√£o `join`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDoo3YsDZ156"
      },
      "source": [
        "Aqui est√° um exemplo simples de corre√ß√£o ortogr√°fica em `Python` usando a biblioteca `autocorrect`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efTvA0gtZR4D",
        "outputId": "19efc798-83c7-4e04-9372-29ba87d78178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/622.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622364 sha256=ae9d8cbed37b469594dd2f6bd7362a4fd1607e1a394fbdd1eecacf078fdfcddb\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/28/c2/9ddf8f57f871b55b6fd0ab99c887531fb9a66e5ff236b82aee\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n"
          ]
        }
      ],
      "source": [
        "# instalar a biblioteca autocorrect\n",
        "!pip install autocorrect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8Nt6RvDZPeV",
        "outputId": "43e26a11-b4e0-4516-b37a-2ec040d22ee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dictionary for this language not found, downloading...\n",
            "__________________________________________________\n",
            "couldn't download http://ipfs.io/ipfs/QmbRSZvfJV6zN12zzWhecphcvE9ZBeQdAJGQ9c9ttJXzcg/pt.tar.gz, trying next url...\n",
            "__________________________________________________\n",
            "couldn't download https://gateway.pinata.cloud/ipfs/QmbRSZvfJV6zN12zzWhecphcvE9ZBeQdAJGQ9c9ttJXzcg/pt.tar.gz, trying next url...\n",
            "__________________________________________________\n",
            "couldn't download https://cf-ipfs.com/ipfs/QmbRSZvfJV6zN12zzWhecphcvE9ZBeQdAJGQ9c9ttJXzcg/pt.tar.gz, trying next url...\n",
            "__________________________________________________\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Este √© um exemplo de frase com erro de ortografia.\n"
          ]
        }
      ],
      "source": [
        "from autocorrect import Speller\n",
        "\n",
        "# criar um objeto Speller com o idioma \"Portuguese\"\n",
        "corrector = Speller(lang='pt')\n",
        "\n",
        "# texto com um erro de ortografia\n",
        "texto = \"Este √© um exempel de frasi com errro de ortografia.\"\n",
        "\n",
        "# corrigir o erro de ortografia no texto\n",
        "texto_corrigido = corrector(texto)\n",
        "\n",
        "# imprimir o texto corrigido\n",
        "print(texto_corrigido)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJmUHTmjtaNa"
      },
      "source": [
        "## **Capitaliza√ß√£o**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDD7hanr_SMK"
      },
      "source": [
        "Processo de colocar os tokens em letra min√∫scula para normaliza√ß√£o do texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4jkS59hrZ8W",
        "outputId": "cba039b5-058c-421b-f08b-806b7764d64d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYp6P1WX_Vhw",
        "outputId": "c6dc0efb-0032-46cc-9625-0d679814c4ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14 ['no', 'meio', 'do', 'caminho', 'tinha', 'uma', 'pedra', 'tinha', 'uma', 'pedra', 'no', 'meio', 'do', 'caminho'] \n",
            " 7 {'do', 'uma', 'pedra', 'no', 'meio', 'caminho', 'tinha'}\n"
          ]
        }
      ],
      "source": [
        "versos = \"\"\"No meio do caminho tinha uma pedra\n",
        "Tinha uma pedra no meio do caminho\"\"\".lower()\n",
        "\n",
        "palavras = nltk.word_tokenize(versos, language='portuguese')\n",
        "\n",
        "print(len(palavras), palavras, '\\n', len(set(palavras)), set(palavras))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XirjLq-9tcqJ"
      },
      "source": [
        "## **Segmenta√ß√£o de Senten√ßas**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJBRLIbetf5F",
        "outputId": "e4415323-269e-4de7-e63d-e8140d2b635f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Eu estou na aula de Processamento de Linguagem Natural.',\n",
              " 'O exemplo √© muito simples.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "\n",
        "texto = \"\"\"Eu estou na aula de Processamento de Linguagem Natural.\n",
        "O exemplo √© muito simples.\"\"\"\n",
        "\n",
        "nltk.sent_tokenize(texto, language='portuguese')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRDS3RxoapDM"
      },
      "source": [
        "## **Radicaliza√ß√£o (Stemiza√ß√£o)**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUT1d7_3kDed"
      },
      "source": [
        "A **stemiza√ß√£o** (*stemming*) √© um processo de normaliza√ß√£o de palavras em lingu√≠stica computacional e processamento de linguagem natural. O objetivo da **stemiza√ß√£o** √© reduzir as palavras em sua forma raiz ou *stem*, de modo que palavras semelhantes possam ser agrupadas juntas. Isso √© √∫til em aplica√ß√µes como an√°lise de sentimentos, recupera√ß√£o de informa√ß√µes e classifica√ß√£o de documentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQCnDpuLNOiC"
      },
      "source": [
        "O processo de **stemiza√ß√£o** envolve a remo√ß√£o de sufixos e prefixos das palavras, deixando apenas o n√∫cleo ou raiz da palavra. Por exemplo, a palavra `corrida` pode ser reduzida ao seu *stem* `corrid`, que pode ser usado para agrupar palavras relacionadas, como `correr`, `corredor` e `corredora`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nMCXElsNOkx"
      },
      "source": [
        "Existem diferentes algoritmos e t√©cnicas para realizar a **stemiza√ß√£o**, como o algoritmo de `Porter`, o `Snowball` e o algoritmo de `Lancaster`. Cada um desses algoritmos tem suas pr√≥prias regras e heur√≠sticas para reduzir as palavras √† sua forma raiz. A escolha do algoritmo depende da l√≠ngua em que o processamento ser√° feito e da aplica√ß√£o espec√≠fica em quest√£o."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnMBuq5TlS49"
      },
      "source": [
        "Aqui est√° um exemplo de c√≥digo em `Python` usando a biblioteca `NLTK` para realizar a stemiza√ß√£o de palavras em portugu√™s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPT4nugStArn",
        "outputId": "5f08b31a-927e-4184-c12c-285551e7d3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2urXssXlMRk"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"portuguese\")\n",
        "\n",
        "def stem_texto(texto):\n",
        "    words = nltk.word_tokenize(texto, language='portuguese')\n",
        "    stems = [stemmer.stem(word) for word in words]\n",
        "    stemmed_text = \" \".join(stems)\n",
        "    return stemmed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxTOblOcN3bw",
        "outputId": "60743740-541e-4d44-f8d8-c201a974d9f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "estou corr no parqu e vi um corredor corr mais r√°p do que eu .\n"
          ]
        }
      ],
      "source": [
        "texto = \"Estou correndo no parque e vi um corredor correndo mais r√°pido do que eu.\"\n",
        "texto_stemmed = stem_texto(texto)\n",
        "\n",
        "print(texto_stemmed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9PwonqMmt0s"
      },
      "source": [
        "Aqui est√° outro exemplo em `Python` usando a biblioteca `NLTK` para realizar a stemiza√ß√£o em portugu√™s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ekmTbgHmg1x",
        "outputId": "4feb9e26-1da0-464e-d49f-6df1303f2069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('rslp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYlfq1C807JE",
        "outputId": "bc8bef04-16ec-4c70-af23-e54174bd5f93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'com', 'est', 'gost']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "raiz = nltk.stem.RSLPStemmer()\n",
        "\n",
        "tokens = nltk.word_tokenize('A comida estava gostosa', language='portuguese')\n",
        "[raiz.stem(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7zkN72T0-6q"
      },
      "source": [
        "Mais um exemplo usando RSLP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtI86nlmmeEX"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "stemmer = RSLPStemmer()\n",
        "\n",
        "def stem_texto(texto):\n",
        "    words = nltk.word_tokenize(texto)\n",
        "    stems = [stemmer.stem(word) for word in words]\n",
        "    stemmed_text = \" \".join(stems)\n",
        "    return stemmed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNovvTzqOH3C",
        "outputId": "08fd11ea-dcf1-4185-e092-13d8d5be41e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eu est corr no parqu e vi um corr corr mais r√°pid do que eu .\n"
          ]
        }
      ],
      "source": [
        "texto = \"Eu estou correndo no parque e vi um corredor correndo mais r√°pido do que eu.\"\n",
        "texto_stemmed = stem_texto(texto)\n",
        "\n",
        "print(texto_stemmed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4yHflh6nADK"
      },
      "source": [
        "Observe que a stemiza√ß√£o n√£o √© perfeita e, √†s vezes, pode levar a resultados imprecisos. Al√©m disso, a stemiza√ß√£o √© uma forma de pr√©-processamento de texto e, portanto, deve ser aplicada antes de outras etapas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-ySihr3lcVB"
      },
      "source": [
        "Aqui est√° um exemplo de c√≥digo em `Python` usando a biblioteca `NLTK` para realizar a stemiza√ß√£o de palavras em ingl√™s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxfFKM4KlYER"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_text(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stems = [stemmer.stem(word) for word in words]\n",
        "    stemmed_text = \" \".join(stems)\n",
        "    return stemmed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoxZhvDfO0ng",
        "outputId": "d5c31c2b-5146-49b6-c0ce-fc06f1d7b64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am run in the park and saw a runner run faster than me .\n"
          ]
        }
      ],
      "source": [
        "text = \"I am running in the park and saw a runner running faster than me.\"\n",
        "stemmed_text = stem_text(text)\n",
        "\n",
        "print(stemmed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y32Pry14rMIT"
      },
      "source": [
        "Aqui est√° outro exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjVqJKVZrJGA",
        "outputId": "cbe332b7-85f0-4d01-82bf-185dea0bbbd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute --> comput\n",
            "computer --> comput\n",
            "computed --> comput\n",
            "computing --> comput\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "tokens = ['compute', 'computer', 'computed', 'computing']\n",
        "\n",
        "for token in tokens:\n",
        "    print(token + ' --> ' + stemmer.stem(token))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK4dHF4JawmS"
      },
      "source": [
        "## **Lematiza√ß√£o**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkLsonzskbwI"
      },
      "source": [
        "A **lematiza√ß√£o** (*lemmatization*) √© um processo de normaliza√ß√£o de palavras em lingu√≠stica computacional e processamento de linguagem natural. O objetivo da **lematiza√ß√£o** √© reduzir palavras em sua forma can√¥nica ou base, conhecida como *lemma*, de modo que palavras com significados semelhantes possam ser agrupadas juntas. Isso √© √∫til em aplica√ß√µes como an√°lise de sentimentos, recupera√ß√£o de informa√ß√µes e classifica√ß√£o de documentos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xThyi9E9PEdI"
      },
      "source": [
        "Ao contr√°rio da stemiza√ß√£o, a **lematiza√ß√£o** n√£o apenas remove os sufixos e prefixos das palavras, mas tamb√©m leva em considera√ß√£o a classe gramatical da palavra e sua flex√£o. Por exemplo, a palavra `correndo` pode ser reduzida ao seu *lemma* `correr`, enquanto a palavra `corrida` pode ser reduzida ao seu *lemma* `corrida`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmjytwzbPGfk"
      },
      "source": [
        "Existem diferentes algoritmos e t√©cnicas para realizar a **lematiza√ß√£o**, como o algoritmo `WordNet` e o algoritmo de **lematiza√ß√£o** baseado em regras. Cada algoritmo tem suas pr√≥prias regras e heur√≠sticas para reduzir as palavras ao seu lemma. A escolha do algoritmo tamb√©m depende da l√≠ngua em que o processamento ser√° feito e da aplica√ß√£o espec√≠fica em quest√£o."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y41CLp9orgk9"
      },
      "source": [
        "Aqui est√° um exemplo de c√≥digo em `Python` usando a biblioteca `NLTK` para realizar a lematiza√ß√£o:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "742czm8BrnJF",
        "outputId": "1bf81074-d7e7-4a6e-862b-c5a94b0c1cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# baixar o corpus do WordNet, se ainda nao foi baixado\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQO7Okh9r4pF"
      },
      "source": [
        "O c√≥digo `nltk.download('wordnet')` faz o download do c√≥rpus do `WordNet`, que √© um dicion√°rio l√©xico online usado para a lematiza√ß√£o, a consulta de sin√¥nimos e outras tarefas relacionadas ao PLN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpGiymtvsJEB"
      },
      "source": [
        "O c√≥digo `nltk.download('omw-1.4')` faz o download do c√≥rpus do *Open Multilingual WordNet* (OMW), que √© um recurso lingu√≠stico que oferece sin√¥nimos em v√°rios idiomas. O `OMW` √© baseado no `WordNet`, mas em vez de se concentrar exclusivamente no ingl√™s, ele inclui sin√¥nimos em v√°rias l√≠nguas, como franc√™s, italiano, espanhol e outros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K48DRnqariYU"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text = \"Os gatos est√£o correndo e brincando juntos\"\n",
        "\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "lemmas = []\n",
        "for token in tokens:\n",
        "    lemma = lemmatizer.lemmatize(token)\n",
        "    lemmas.append(lemma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz7WxfVtQwcX",
        "outputId": "9abd8951-ed2f-4dd2-e924-975428e225a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Os', 'gatos', 'est√£o', 'correndo', 'e', 'brincando', 'junto']\n"
          ]
        }
      ],
      "source": [
        "print(lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsS7Btof6Nbh"
      },
      "source": [
        "Aqui est√° um exemplo de como realizar a lematiza√ß√£o usando a biblioteca `spaCy` em `Python`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fGFVTomxUX3",
        "outputId": "51d97ad6-880b-42c5-8a4f-727477fc299e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.8.0/pt_core_news_lg-3.8.0-py3-none-any.whl (568.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pt-core-news-lg\n",
            "Successfully installed pt-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_lg')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download pt_core_news_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSP0SwyG6BpS"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# carregar o modelo de linguagem em portugues\n",
        "nlp = spacy.load('pt_core_news_lg')\n",
        "\n",
        "# texto de exemplo\n",
        "texto = \"Eu estava correndo no parque quando vi um bando de p√°ssaros.\"\n",
        "\n",
        "# processar o texto com o modelo de linguagem\n",
        "doc = nlp(texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9BnkljlRVx_",
        "outputId": "4254c23a-1971-4b55-81fd-2dd1a606f1e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu eu\n",
            "estava estar\n",
            "correndo correr\n",
            "no em o\n",
            "parque parque\n",
            "quando quando\n",
            "vi ver\n",
            "um um\n",
            "bando bando\n",
            "de de\n",
            "p√°ssaros p√°ssaro\n",
            ". .\n"
          ]
        }
      ],
      "source": [
        "# imprimir as palavras e seus lemas\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvL8fq6f6rUJ"
      },
      "source": [
        "Observe que as palavras `estava` e `correndo` foram reduzidas ao seu lema `estar` e `correr`, respectivamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xi9KXYE7oSj"
      },
      "source": [
        "Aqui est√° um exemplo de c√≥digo para a lematiza√ß√£o e stemiza√ß√£o de palavras usando a biblioteca `NLTK` em Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efAZzc3v7jSz"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "\n",
        "# instanciar um objeto lematizador do WordNet\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# instanciar um objeto stemizador para o idioma portugu√™s\n",
        "stemmer = SnowballStemmer(\"portuguese\")\n",
        "\n",
        "# texto de exemplo\n",
        "texto = \"Os cachorros correm no parque enquanto os p√°ssaros cantam.\"\n",
        "\n",
        "# realizar a tokenizacao do texto\n",
        "tokens = nltk.word_tokenize(texto)\n",
        "\n",
        "# inicializar listas para armazenar as palavras lematizadas e stemizadas\n",
        "lemas = []\n",
        "stems = []\n",
        "\n",
        "# percorrer cada token e realizar a lematiza√ß√£o e a stemiza√ß√£o\n",
        "for token in tokens:\n",
        "    # lematizacao\n",
        "    lemma = lemmatizer.lemmatize(token)\n",
        "    lemas.append(lemma)\n",
        "\n",
        "    # stemizacao\n",
        "    stem = stemmer.stem(token)\n",
        "    stems.append(stem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VWu2paLStg4",
        "outputId": "a2f37011-3050-4218-aaa2-ee12fa79f27c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original:  Os cachorros correm no parque enquanto os p√°ssaros cantam.\n",
            "Lematizado:  Os cachorros correm no parque enquanto o p√°ssaros cantam .\n",
            "Stemizado:  os cachorr corr no parqu enquant os p√°ssar cant .\n"
          ]
        }
      ],
      "source": [
        "# imprimir o texto lematizado e stemizado\n",
        "print(\"Texto original: \", texto)\n",
        "print(\"Lematizado: \", \" \".join(lemas))\n",
        "print(\"Stemizado: \", \" \".join(stems))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh4gud-2yK5Q",
        "outputId": "159b00df-77d8-4f46-fe5b-9243eff7ce09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SONf_cO074uH"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from collections import Counter\n",
        "\n",
        "# instanciar um objeto lematizador do WordNet\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# instanciar um objeto stemizador para o idioma ingles\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# definir o texto de exemplo\n",
        "texto = \"The quick brown fox jumped over the lazy dog. The dog was not amused.\"\n",
        "\n",
        "# realizar a tokeniza√ß√£o do texto e remover as stopwords\n",
        "tokens = [token.lower() for token in word_tokenize(texto) if token.isalpha() and token.lower() not in stopwords.words(\"english\")]\n",
        "\n",
        "# inicializa listas para armazenar as palavras lematizadas e stemizadas\n",
        "lemas = []\n",
        "stems = []\n",
        "\n",
        "# percorrer cada token e realizar a lematizacao e a stemizacao\n",
        "for token in tokens:\n",
        "    # lematizacao\n",
        "    lemma = lemmatizer.lemmatize(token)\n",
        "    lemas.append(lemma)\n",
        "\n",
        "    # stemizacao\n",
        "    stem = stemmer.stem(token)\n",
        "    stems.append(stem)\n",
        "\n",
        "# criar um contador para contar a frequencia de cada palavra\n",
        "frequencias = Counter(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCx7hoZNTPZW",
        "outputId": "e6966236-a87a-47c7-86f3-929bb853d5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original:  The quick brown fox jumped over the lazy dog. The dog was not amused.\n",
            "Tokens:  ['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog', 'dog', 'amused']\n",
            "Palavras unicas:  {'quick', 'amused', 'fox', 'jumped', 'brown', 'lazy', 'dog'}\n",
            "Frequencias:  Counter({'dog': 2, 'quick': 1, 'brown': 1, 'fox': 1, 'jumped': 1, 'lazy': 1, 'amused': 1})\n",
            "Lemas:  ['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog', 'dog', 'amused']\n",
            "Stems:  ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog', 'dog', 'amus']\n"
          ]
        }
      ],
      "source": [
        "# imprimir os resultados\n",
        "print(\"Texto original: \", texto)\n",
        "print(\"Tokens: \", tokens)\n",
        "print(\"Palavras unicas: \", set(tokens))\n",
        "print(\"Frequencias: \", frequencias)\n",
        "print(\"Lemas: \", lemas)\n",
        "print(\"Stems: \", stems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrCI4xx38GqP"
      },
      "source": [
        "Observe que a fun√ß√£o remove as *stopwords* (palavras comuns que n√£o agregam muito significado ao texto, como `the` e `was`) antes de realizar a lematiza√ß√£o e a stemiza√ß√£o. Al√©m disso, o c√≥digo utiliza um objeto `Counter` do `Python` para contar a frequ√™ncia de cada palavra no texto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb-grWGcBsBx"
      },
      "source": [
        "## **Normaliza√ß√£o de Textos**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-0b48jvES78"
      },
      "source": [
        "**Normaliza√ß√£o de textos** √© o processo de transformar textos em uma forma mais consistente e uniforme. O objetivo da normaliza√ß√£o √© tornar o texto mais f√°cil de ser processado por algoritmos, melhorar a qualidade da an√°lise de texto e tornar as compara√ß√µes de palavras e frases mais precisas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S5qOqTzTsw1"
      },
      "source": [
        "A **normaliza√ß√£o** pode incluir a remo√ß√£o de caracteres especiais, a corre√ß√£o ortogr√°fica, a padroniza√ß√£o de mai√∫sculas e min√∫sculas, a remo√ß√£o de pontua√ß√£o, a remo√ß√£o de *stopwords*  e a lematiza√ß√£o/stemiza√ß√£o.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7-o6uxJT6WJ"
      },
      "source": [
        "A **normaliza√ß√£o de texto** √© uma tarefa importante em v√°rias aplica√ß√µes de PLN, incluindo an√°lise de sentimento, classifica√ß√£o de texto, tradu√ß√£o autom√°tica, reconhecimento de fala etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbODIN9dCD54"
      },
      "source": [
        "Aqui est√° um exemplo de normaliza√ß√£o de texto que combina o uso do m√≥dulo `unicodedata` do `Python` com express√µes regulares:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIdGL9JBBZwb",
        "outputId": "988f8d55-ef4c-4847-dc9d-9bb844f776fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ola! Tudo bem? Eu gostaria de pedir um cafe expresso, por favor.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# definir um texto com caracteres acentuados, espa√ßos extras e caracteres especiais\n",
        "texto = \"    Ol√°! Tudo bem? Eu gostaria de pedir um caf√© expresso, por favor.   ‚òïÔ∏è \"\n",
        "\n",
        "# remover caracteres especiais e acentos utilizando o modulo unicodedata\n",
        "texto_sem_acentos = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('ASCII')\n",
        "\n",
        "# remover espacos extras utilizando expressoes regulares\n",
        "texto_normalizado = re.sub(r'\\s+', ' ', texto_sem_acentos).strip()\n",
        "\n",
        "# imprimir o texto normalizado\n",
        "print(texto_normalizado)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKKoSC3dCN9U"
      },
      "source": [
        "Esse processo de normaliza√ß√£o pode ser √∫til para padronizar o texto e reduzir a varia√ß√£o de caracteres em diferentes fontes de dados. Ele tamb√©m pode ajudar a simplificar a an√°lise de texto, especialmente quando h√° diferen√ßas de codifica√ß√£o entre as fontes de dados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUbNIGbeCVKQ"
      },
      "source": [
        "Aqui est√° um exemplo de normaliza√ß√£o de texto que utiliza express√µes regulares para substituir padr√µes espec√≠ficos de caracteres:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQQ_pWM0Bxzp",
        "outputId": "8c142ccb-9cde-4bda-e714-f53083f1d329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eu n√£o sei se voc√™ vai comigo, mas acho que a turma vai. Vou te mandar um mensagem depois\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# definir um texto com abreviacoes e girias\n",
        "texto = \"Eu n sei se vc vai cmg, mas acho q a galera vai. Vou t mandar um zap dps\"\n",
        "\n",
        "# definir um dicionario de substituicao\n",
        "substituicoes = {\n",
        "    r'n sei': 'n√£o sei',\n",
        "    r'vc': 'voc√™',\n",
        "    r'cmg': 'comigo',\n",
        "    r'q': 'que',\n",
        "    r'galera': 'turma',\n",
        "    r't ': 'te ',\n",
        "    r'zap': 'mensagem',\n",
        "    r'dps': 'depois'\n",
        "}\n",
        "\n",
        "# iterar sobre as chaves do dicionario e aplicar as substituicoes com expressoes regulares\n",
        "for padrao, substituicao in substituicoes.items():\n",
        "    texto = re.sub(padrao, substituicao, texto)\n",
        "\n",
        "# imprimir o resultado\n",
        "print(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzPxKyUgCbOJ"
      },
      "source": [
        "Nesse exemplo, express√µes regulares s√£o usadas para substituir abrevia√ß√µes e g√≠rias por palavras completas. O dicion√°rio `substituicoes` cont√©m as chaves que representam os padr√µes de caracteres que ser√£o substitu√≠dos e os valores que representam as palavras completas que ser√£o utilizadas como substitui√ß√£o. Em seguida, iteramos sobre as chaves do dicion√°rio e aplicamos as substitui√ß√µes com a fun√ß√£o `re.sub()` da biblioteca `re`. O resultado √© um texto normalizado que utiliza palavras completas em vez de abrevia√ß√µes e g√≠rias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MpRQ8x0HVrc"
      },
      "source": [
        "Aqui est√° outro exemplo de normaliza√ß√£o de texto em `Python`, usando a biblioteca `re` e a t√©cnica de remo√ß√£o de caracteres especiais:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jv9JfuxCHRpA",
        "outputId": "6ba8ecb0-482b-4c85-bb44-a774de13e6ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boa noite \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# definir uma string com caracteres especiais\n",
        "texto = \"Boa noite!!! üòÉüéâüî•üéÇüëãüèºüë®üèª‚Äçüíª\"\n",
        "\n",
        "# remover caracteres especiais\n",
        "texto_sem_caracteres_especiais = re.sub(r'[^\\w\\s]', '', texto)\n",
        "\n",
        "# imprimir o texto normalizado\n",
        "print(texto_sem_caracteres_especiais)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4s7B64bIYHk",
        "outputId": "a3de25b8-ab44-42ec-ffe3-aecaa602a211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acabei de assistir um filme incr√≠vel!  voc√™s arrasaram! üé•üçø \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# definir um tweet para normalizar\n",
        "tweet = \"Acabei de assistir um filme incr√≠vel! @Netflix, voc√™s arrasaram! üé•üçø http://netflix.com/filme\"\n",
        "\n",
        "# remover URLs e mencoes\n",
        "tweet_sem_url = re.sub(r\"http\\S+\", \"\", tweet) # remover URLs\n",
        "tweet_sem_mencao = re.sub(r\"@\\S+\", \"\", tweet_sem_url) # remover men√ß√µes\n",
        "\n",
        "# imprimir o tweet normalizado\n",
        "print(tweet_sem_mencao)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqNnqLgBIpel",
        "outputId": "9a8fc0e0-2bbb-4e3d-cc16-8b3d9c2479d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['esta', 'frase', 'tem', 'muita', 'pontua√ß√£o', 'certo', 'mas', 'vamos', 'normalizar']\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "# definir um texto para normalizar\n",
        "texto = \"Esta frase tem! muita pontua√ß√£o, certo? Mas vamos normalizar.\"\n",
        "\n",
        "# transformar todas as letras em minusculas\n",
        "texto = texto.lower()\n",
        "\n",
        "# tokenizar o texto e remover a pontuacao\n",
        "tokens = texto.split()\n",
        "tokens_sem_pontuacao = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
        "\n",
        "# imprimir os tokens normalizados\n",
        "print(tokens_sem_pontuacao)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDSQWPNiJkDH",
        "outputId": "76d74964-f723-4732-d3ac-9dc31526e6b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A reuni√£o est√° marcada para o dia 2022-03-15.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# definir um texto em portugues com uma data\n",
        "texto = \"A reuni√£o est√° marcada para o dia 15/03/2022.\"\n",
        "\n",
        "# normalizar a data para o formato 'YYYY-MM-DD'\n",
        "padrao_data = r'(\\d{2})/(\\d{2})/(\\d{4})'\n",
        "texto_normalizado = re.sub(padrao_data, r'\\3-\\2-\\1', texto)\n",
        "\n",
        "# imprimir o texto normalizado\n",
        "print(texto_normalizado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBVhgPZlJ78H",
        "outputId": "7d8b9797-d7af-47f5-f646-68edfedf27ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu n√∫mero de telefone √© 11999997777.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# definir um texto em portugues com um numero de telefone\n",
        "texto = \"Meu n√∫mero de telefone √© (11) 99999-7777.\"\n",
        "\n",
        "# normalizar o numero de telefone para o formato '11999999999'\n",
        "padrao_telefone = r'\\((\\d{2})\\)\\s+(\\d{5})-(\\d{4})'\n",
        "texto_normalizado = re.sub(padrao_telefone, r'\\1\\2\\3', texto)\n",
        "\n",
        "# imprimir o texto normalizado\n",
        "print(texto_normalizado)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4rV2LhGNc8yAODon675f+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}