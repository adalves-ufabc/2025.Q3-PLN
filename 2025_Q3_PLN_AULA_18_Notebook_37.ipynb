{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adalves-ufabc/2025.Q3-PLN/blob/main/2025_Q3_PLN_AULA_18_Notebook_37.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2025-Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmK05FgcOzL2"
      },
      "source": [
        "## **Agentes e RAG [LangChain]**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma das aplicações mais poderosas possibilitadas pelos LLMs são os chatbots sofisticados de perguntas e respostas (*Q&A*). Essas aplicações conseguem responder a perguntas sobre informações específicas de uma fonte. Elas utilizam uma técnica conhecida como Geração Aumentada por Recuperação (*RAG*, na sigla em inglês)."
      ],
      "metadata": {
        "id": "7RLHnmDqpc7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este exemplo mostrará como construir uma aplicação simples de *Q&A* a partir de uma fonte de dados de texto não estruturado. Demonstraremos:\n",
        "\n",
        "* Um agente RAG que executa buscas com uma ferramenta simples. Esta é uma boa implementação de propósito geral.\n",
        "\n",
        "* Uma cadeia RAG de duas etapas que utiliza apenas uma única chamada de LLM por consulta. Este é um método rápido e eficaz para consultas simples."
      ],
      "metadata": {
        "id": "oGsdiBCvqM5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conceitos**\n",
        "\n",
        "\n",
        "Abordaremos os seguintes conceitos:\n",
        "\n",
        "**Indexação**: um pipeline para ingerir dados de uma fonte e indexá-los. Isso geralmente ocorre em um processo separado.\n",
        "\n",
        "**Recuperação e geração**: o processo RAG propriamente dito, que recebe a consulta do usuário em tempo de execução e recupera os dados relevantes do índice, passando-os em seguida para o modelo.\n",
        "\n",
        "Após indexarmos nossos dados, usaremos um agente como nossa estrutura de orquestração para implementar as etapas de recuperação e geração."
      ],
      "metadata": {
        "id": "FtRz0S7QqfD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **LangChain**"
      ],
      "metadata": {
        "id": "K9S2oK1wYIRd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYKEbnlNTVlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f448f77-8e84-439a-dc65-58f498c67348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.8/93.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Instalando o pacote LangChain\n",
        "\n",
        "!pip install -qU langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJvs5RTcbE64",
        "outputId": "b024605c-7579-479e-ce1b-ace03f5513cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0.5\n"
          ]
        }
      ],
      "source": [
        "#@title Versão do LangChain\n",
        "\n",
        "import langchain\n",
        "\n",
        "print(langchain.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Integração com o pacote da OpenAI\n",
        "\n",
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "8klfbjqKbUpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RM5RnKClbPtQ",
        "outputId": "031d5abd-0f24-4a54-82f0-e3a8bbbc8f6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "#@title Definindo a chave da API da OpenAI\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Definindo a chave da API da OpenAI\n",
        "from getpass import getpass\n",
        "\n",
        "OPENAI_API_KEY = getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDIhzSmjQNxK",
        "outputId": "d4c71ebb-13a9-4a08-b3c8-b375ad464f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exemplo**"
      ],
      "metadata": {
        "id": "SeaCbCSyYcFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neste exemplo, criaremos um aplicativo que responde a perguntas sobre o conteúdo do site. O site específico que usaremos é a postagem do blog *LLM Powered Autonomous Agents*, de Lilian Weng, que nos permite fazer perguntas sobre o conteúdo da postagem."
      ],
      "metadata": {
        "id": "gp4IwoXcq40u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este exemplo requer as seguintes dependências:"
      ],
      "metadata": {
        "id": "7SjXqrb8rHFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-text-splitters langchain-community langchain-core bs4"
      ],
      "metadata": {
        "id": "JCiMivPgpldN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defina um modelo:"
      ],
      "metadata": {
        "id": "FxlPY0xxcJ_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "modelo = ChatOpenAI(model=\"gpt-4.1\", api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "C2yjn1KUrcFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecione um modelo de *embeddings*:"
      ],
      "metadata": {
        "id": "x5WdiY_KrvHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "o_DP1sSAry7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seleciona um *vector store*:"
      ],
      "metadata": {
        "id": "fodMJj2EsAkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "metadata": {
        "id": "MWNmJ5JnsGe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carregando documentos**\n",
        "\n",
        "Primeiro, precisamos carregar o conteúdo da postagem do blog. Podemos usar **`DocumentLoaders`** para isso, que são objetos que carregam dados de uma fonte e retornam uma lista de objetos **`Document`**. Neste caso, usaremos o **`WebBaseLoader`**, que usa o `urllib` para carregar HTML de URLs da web e o `BeautifulSoup` para analisá-lo e convertê-lo em texto. Podemos personalizar a análise de HTML para texto passando parâmetros para o analisador `BeautifulSoup` por meio de `bs_kwargs`. Neste caso, apenas as tags HTML com as classes `post-content`, `post-title` ou `post-header` são relevantes, então removeremos todas as outras."
      ],
      "metadata": {
        "id": "sCX_-s5ncWmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Only keep post title, headers, and content from the full HTML.\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "assert len(docs) == 1\n",
        "print(f\"Total characters: {len(docs[0].page_content)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHpFDgmWcW1w",
        "outputId": "0dc285ee-0093-4583-f7a2-13e38d84e171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 43047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axRhxNKQclhC",
        "outputId": "f98d7a0a-ba5e-4045-bcff-ad93a0b36a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dividindo documentos**\n",
        "\n",
        "Nosso documento carregado tem mais de 42k caracteres, o que é muito longo para caber na janela de contexto de muitos modelos. Mesmo para aqueles modelos que conseguem exibir a postagem completa em sua janela de contexto, eles podem ter dificuldades para encontrar informações em entradas muito longas.\n",
        "\n",
        "Para lidar com isso, dividiremos o **`Document`** em partes para *embedding*  e armazenamento vetorial (*vector storage*). Isso deve nos ajudar a recuperar apenas as partes mais relevantes da postagem do blog em tempo de execução.\n",
        "\n",
        "Usaremos um `RecursiveCharacterTextSplitter`, que dividirá o documento recursivamente usando separadores comuns, como novas linhas, até que cada parte tenha o tamanho apropriado. Este é o divisor de texto recomendado para casos de uso de texto genérico."
      ],
      "metadata": {
        "id": "1wKrXmywctxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # chunk size (characters)\n",
        "    chunk_overlap=200,  # chunk overlap (characters)\n",
        "    add_start_index=True,  # track index in original document\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9vNjkCMcuIb",
        "outputId": "73af406b-4d91-4d97-f9dc-384ff1875186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split blog post into 63 sub-documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Armazenando documentos**\n",
        "\n",
        "Agora precisamos indexar nossos 63 blocos de texto para que possamos pesquisá-los em tempo de execução. Nossa abordagem é fazer o *`embed`* do conteúdo de cada divisão de documento e inserir essas incorporações em um armazenamento vetorial (*`vector store`*). Dada uma consulta de entrada, podemos então usar a busca vetorial para recuperar os documentos relevantes."
      ],
      "metadata": {
        "id": "gYm_pj3Bcx2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_ids = vector_store.add_documents(documents=all_splits)\n",
        "\n",
        "print(document_ids[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odpVdyMmcyRK",
        "outputId": "6003e519-1ec9-4faa-82fd-65d00b0b49e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['f9a542e6-3b5e-40b9-a1ef-59f02c9a3207', '2cd7c05a-6e48-412c-b7e8-0620006c93e4', '024fa0f8-3dec-4168-9b3e-2069aa3a41d3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As aplicações `RAG` geralmente funcionam da seguinte maneira:\n",
        "\n",
        "**Recuperação**: Dada uma entrada do usuário, as divisões relevantes são recuperadas do armazenamento usando um `Retriever`.\n",
        "\n",
        "**Geração**: Um modelo produz uma resposta usando um prompt que inclui tanto a pergunta quanto os dados recuperados."
      ],
      "metadata": {
        "id": "50rl_K_Tc1eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agentes RAG**\n",
        "\n",
        "Uma formulação de uma aplicação `RAG` é como um agente simples com uma ferramenta que recupera informações. Podemos montar um agente `RAG` mínimo implementando uma ferramenta que encapsula nosso armazenamento vetorial:"
      ],
      "metadata": {
        "id": "BgCHmb1ChXpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool(response_format=\"content_and_artifact\")\n",
        "def retrieve_context(query: str):\n",
        "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
        "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
        "    serialized = \"\\n\\n\".join(\n",
        "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
        "        for doc in retrieved_docs\n",
        "    )\n",
        "    return serialized, retrieved_docs"
      ],
      "metadata": {
        "id": "AzGc1FiCc1ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com a ferramenta que temos, podemos construir o agente:"
      ],
      "metadata": {
        "id": "VGJQh4hvc5zF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_agent\n",
        "\n",
        "\n",
        "tools = [retrieve_context]\n",
        "# If desired, specify custom instructions\n",
        "prompt = (\n",
        "    \"You have access to a tool that retrieves context from a blog post.\"\n",
        "    \"Use the tool to help answer user queries.\"\n",
        ")\n",
        "agente = create_agent(modelo, tools, system_prompt=prompt)"
      ],
      "metadata": {
        "id": "Ai6eiX8EdOmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos testar isso. Criamos uma pergunta que normalmente exigiria uma sequência iterativa de etapas de recuperação para ser respondida:"
      ],
      "metadata": {
        "id": "A_Seu64pd0jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = (\n",
        "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
        "    \"Once you get the answer, look up common extensions of that method.\"\n",
        ")\n",
        "\n",
        "for event in agente.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT09GwzId0zz",
        "outputId": "ab02d9b2-edf2-42af-94b8-3b5e44c7efbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is the standard method for Task Decomposition?\n",
            "\n",
            "Once you get the answer, look up common extensions of that method.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  retrieve_context (call_4APjHGJSJRWmGpgeFo2fFz82)\n",
            " Call ID: call_4APjHGJSJRWmGpgeFo2fFz82\n",
            "  Args:\n",
            "    query: standard method for Task Decomposition\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: retrieve_context\n",
            "\n",
            "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
            "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
            "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
            "Self-Reflection#\n",
            "\n",
            "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
            "Content: Component One: Planning#\n",
            "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
            "Task Decomposition#\n",
            "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
            "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  retrieve_context (call_7ag8OaKyF7zr1Tn5Na8S5o8N)\n",
            " Call ID: call_7ag8OaKyF7zr1Tn5Na8S5o8N\n",
            "  Args:\n",
            "    query: extensions of Chain of Thought task decomposition\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: retrieve_context\n",
            "\n",
            "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
            "Content: Component One: Planning#\n",
            "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
            "Task Decomposition#\n",
            "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
            "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
            "\n",
            "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
            "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
            "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
            "Self-Reflection#\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The standard method for Task Decomposition is the Chain of Thought (CoT) prompting technique. In CoT, the model is instructed to “think step by step,” breaking down complex tasks into smaller, manageable sub-tasks. This helps with both model performance and interpretability by making the model’s reasoning process explicit.\n",
            "\n",
            "Common extensions of Chain of Thought include:\n",
            "\n",
            "- Tree of Thoughts (ToT): This extends CoT by exploring multiple reasoning possibilities at each step. It decomposes problems into multiple steps and generates several thoughts per step, arranging them in a tree structure. The search for solutions in this tree can use strategies like breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by classifiers or majority votes.\n",
            "- LLM+P (using PDDL): Involves translating the problem into a formal planning language (PDDL), using an external classical planner for long-horizon planning, then converting the plan back into natural language. This is common in some robotics and structured planning domains.\n",
            "- Task-specific instructions or human input: Using carefully defined prompts for specific tasks, or directly relying on human guidance for decomposition.\n",
            "\n",
            "Let me know if you need technical details or examples!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe que o agente:\n",
        "\n",
        "1. Gera uma consulta para buscar um método padrão de decomposição de tarefas;\n",
        "\n",
        "2. Ao receber a resposta, gera uma segunda consulta para buscar extensões comuns desse método;\n",
        "\n",
        "3. Tendo recebido todo o contexto necessário, responde à pergunta."
      ],
      "metadata": {
        "id": "2jkxHCSReG3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cadeias RAG**\n",
        "\n",
        "Na formulação RAG agentiva acima, permitimos que o LLM use seu critério para gerar uma chamada de ferramenta que ajude a responder às consultas do usuário."
      ],
      "metadata": {
        "id": "MpU327VYiNFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outra abordagem comum é uma cadeia de duas etapas, na qual sempre executamos uma busca (potencialmente usando a consulta bruta do usuário) e incorporamos o resultado como contexto para uma única consulta LLM. Isso resulta em uma única chamada de inferência por consulta, reduzindo a latência à custa da flexibilidade. Nessa abordagem, não chamamos mais o modelo em um loop, mas fazemos uma única passagem. Podemos implementar essa cadeia removendo ferramentas do agente e, em vez disso, incorporando a etapa de recuperação em um prompt personalizado:"
      ],
      "metadata": {
        "id": "Kj1RDXaeiUgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
        "\n",
        "@dynamic_prompt\n",
        "def prompt_with_context(request: ModelRequest) -> str:\n",
        "    \"\"\"Inject context into state messages.\"\"\"\n",
        "    last_query = request.state[\"messages\"][-1].text\n",
        "    retrieved_docs = vector_store.similarity_search(last_query)\n",
        "\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "\n",
        "    system_message = (\n",
        "        \"You are a helpful assistant. Use the following context in your response:\"\n",
        "        f\"\\n\\n{docs_content}\"\n",
        "    )\n",
        "\n",
        "    return system_message\n",
        "\n",
        "\n",
        "agente = create_agent(modelo, tools=[], middleware=[prompt_with_context])"
      ],
      "metadata": {
        "id": "wp6AqZ_WeHET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos experimentar:"
      ],
      "metadata": {
        "id": "qUnyh-W8ikm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is task decomposition?\"\n",
        "for step in agente.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xoq_AC7eLjB",
        "outputId": "8a35cb40-8314-4f4a-f7c7-6ea889277168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "What is task decomposition?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Task decomposition is the process of breaking down a complex or complicated task into smaller, more manageable sub-tasks or steps. This makes it easier to solve large problems by handling each simple sub-task separately and often in sequence.\n",
            "\n",
            "In AI and large language models (LLMs), task decomposition can be done using simple prompts (like \"List the steps for doing XYZ\"), through task-specific instructions (such as \"Write a story outline\" for novel writing), or with human input. Advanced methods like Chain of Thought (CoT) prompting encourage the model to \"think step by step,\" while Tree of Thoughts creates a branching structure to explore multiple reasoning paths.\n",
            "\n",
            "Another approach involves combining LLMs with external classical planners (using PDDL), where the model translates problems into planning language, asks a planner to generate a multi-step solution, and then converts that plan back into natural language.\n",
            "\n",
            "Overall, task decomposition helps both humans and AI agents effectively plan and solve complex goals by dividing them into logical, sequential actions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Referência:**\n",
        "\n",
        "> https://docs.langchain.com/oss/python/langchain/rag"
      ],
      "metadata": {
        "id": "bCQA3orRioAp"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgDvEljj40CL1m1Qc4sw2B",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}